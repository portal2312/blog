<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Representation - Cleaning Data (데이터 정제) | Portal2312&#39;s blog</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="Representation - Cleaning Data (데이터 정제)" />
<meta name="author" content="mkkim" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Scaling feature values (특성 값 조정) Handling extreme outliers (극단적 이상점 처리) Binning (비닝) Scrubbing (스크러빙) Know your data (철저한 데이터 파악)" />
<meta property="og:description" content="Scaling feature values (특성 값 조정) Handling extreme outliers (극단적 이상점 처리) Binning (비닝) Scrubbing (스크러빙) Know your data (철저한 데이터 파악)" />
<link rel="canonical" href="/blog/docs/develop/A-M/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/009.%20Representation%20(%ED%91%9C%ED%98%84,%2065%20min)/004.%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EC%A0%95%EC%A0%9C.html" />
<meta property="og:url" content="/blog/docs/develop/A-M/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/009.%20Representation%20(%ED%91%9C%ED%98%84,%2065%20min)/004.%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EC%A0%95%EC%A0%9C.html" />
<meta property="og:site_name" content="Portal2312&#39;s blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-12-02T16:38:51+09:00" />
<script type="application/ld+json">
{"description":"Scaling feature values (특성 값 조정) Handling extreme outliers (극단적 이상점 처리) Binning (비닝) Scrubbing (스크러빙) Know your data (철저한 데이터 파악)","headline":"Representation - Cleaning Data (데이터 정제)","dateModified":"2019-12-02T16:38:51+09:00","datePublished":"2019-12-02T16:38:51+09:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"/blog/docs/develop/A-M/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/009.%20Representation%20(%ED%91%9C%ED%98%84,%2065%20min)/004.%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EC%A0%95%EC%A0%9C.html"},"url":"/blog/docs/develop/A-M/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/009.%20Representation%20(%ED%91%9C%ED%98%84,%2065%20min)/004.%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EC%A0%95%EC%A0%9C.html","author":{"@type":"Person","name":"mkkim"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href='/blog/assets/main.css'><link type="application/atom+xml" rel="alternate" href="/blog/feed.xml" title="Portal2312's blog" /><script>
if(!(window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1")) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-113063601-1', 'auto');
  ga('send', 'pageview');
}
</script>
  
<script src='/blog/dist/js/common.bundle.js'></script>
</head>
<body><header class="site-header" role="banner">
  <div class="wrapper">
    <a class="site-title" rel="author" href="/blog/">Portal2312&#39;s blog</a>
    <nav class="site-nav">
    <input type="checkbox" id="nav-trigger" class="nav-trigger" />
    <label for="nav-trigger">
      <span class="menu-icon">
        <svg viewBox="0 0 18 15" width="18px" height="15px">
          <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
        </svg>
      </span>
    </label>

    <div class="trigger"><a class="page-link" href="/blog/about.html">
            About
          </a><a class="page-link" href="/blog/posts.html">
            Posts
          </a><a class="page-link" href="/blog/history.html">
            History
          </a><a class="page-link" href="/blog/docs/index.html">
            Docs
          </a></div>
  </nav>
  </div>
  <div class="scroll-indicator-container">
  <div class="scroll-indicator-bar" id="scrollIndicatorBar"></div>
</div>

</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <h1>Representation - Cleaning Data (데이터 정제)</h1>

  <div>
    <h2>Table of contents</h2>
    <ul class="section-nav">
<li class="toc-entry toc-h3"><a href="#scaling-feature-values">Scaling feature values (특성 값 조정)</a></li>
<li class="toc-entry toc-h3"><a href="#handling-extreme-outliers">Handling extreme outliers (극단적 이상점 처리)</a></li>
<li class="toc-entry toc-h3"><a href="#binning">Binning (비닝)</a></li>
<li class="toc-entry toc-h3"><a href="#scrubbing">Scrubbing (스크러빙)</a></li>
<li class="toc-entry toc-h3"><a href="#know-your-data">Know your data (철저한 데이터 파악)</a></li>
</ul><ul>
<li><a href="#scaling-feature-values-">Scaling feature values (특성 값 조정)</a></li>
<li><a href="#handling-extreme-outliers-">Handling extreme outliers (극단적 이상점 처리)</a></li>
<li><a href="#binning-">Binning (비닝)</a></li>
<li><a href="#scrubbing-">Scrubbing (스크러빙)</a></li>
<li><a href="#know-your-data-">Know your data (철저한 데이터 파악)</a></li>
</ul>

<p>한 그루의 사과나무에서는 좋은 사과와 벌레 먹은 사과가 같이 열립니다.
그러나 과일 가게에서 판매하는 과일들은 모두 최고의 품질을 자랑합니다.
유통 과정 중간에서 누군가가 심혈을 기울여 불량한 과일을 솎아내고 약간 흠집이 있는 과일을 깨끗이 손질한 것입니다.
ML 엔지니어 역시 불량한 예를 솎아내고 약간 문제가 있는 예를 깨끗이 손질하는 데 막대한 노력을 기울입니다.
'나쁜 사과' 몇 개가 거대한 데이터 세트를 망칠 수 있습니다.</p>

<h3 id="scaling-feature-values">
<a class="anchor" href="#scaling-feature-values" aria-hidden="true"><span class="octicon octicon-link"></span></a>Scaling feature values (특성 값 조정)</h3>

<p><strong>조정</strong>이란 부동 소수점 특성 값을 100~900 등의 자연 범위에서 0~1 또는 -1~+1 등의 표준 범위로 변환하는 작업입니다.</p>

<p>특성 세트가 단일 특성으로만 구성된 경우 조정에 따르는 실질적인 이점은 거의 없습니다.
그러나 특성 세트가 여러 특성으로 구성되었다면 특성 조정으로 다음과 같은 이점을 누릴 수 있습니다:</p>

<ul>
<li><p>경사하강법이 더 빠르게 수렴됩니다.</p></li>
<li><p>'NaN 트랩'이 방지됩니다.<br>
NaN 트랩이란 모델의 숫자 중 하나가 NaN(예: 학습 중에 값이 부동 소수점 정밀도 한도를 초과하는 경우)이 된 후 수학 연산 과정에서 모델의 다른 모든 숫자가 결국 NaN이 되는 상황입니다.</p></li>
<li><p>모델이 각 특성의 적절한 가중치를 익히는 데 도움이 됩니다.<br>
특성 조정을 수행하지 않으면 모델에서 범위가 더 넓은 특성을 과도하게 중시합니다.</p></li>
</ul>

<p>모든 부동 소수점 특성에 동일한 척도를 부여할 필요는 없습니다.
특성 A는 -1~+1로, 특성 B는 -3~+3으로 조정해도 심각한 부작용은 없습니다.
그러나 특성 B를 5000~100000으로 조정하면 모델이 부정적으로 반응할 것입니다.</p>

<blockquote>
<p><strong>조정</strong>에 대한 상세내용:</p>

<p>숫자 데이터를 조정하는 알기 쉬운 방법 중 하나는 [최소값, 최대값]을 [-1, +1] 등의 작은 척도로 선형 매핑하는 것입니다.</p>

<p>각 값의 Z 점수를 계산하는 조정 방식도 널리 사용됩니다.<br>
Z 점수는 표준편차를 기준으로 평균에서 벗어난 정도를 계산합니다. 다시 말하면 다음과 같습니다.</p>

<p>scaledvalue = (value - mean)/stddev.</p>

<p>아래와 같은 예를 들어 보겠습니다.</p>

<ul>
<li>평균 = 100</li>
<li>표준편차 = 20</li>
<li>원래 값 = 130</li>
</ul>

<p>결과는 다음과 같습니다.</p>

<p>scaled_value = (130 - 100) / 20
  scaled_value = 1.5</p>

<p>Z 점수로 조정하면 대부분의 조정 값이 -3~+3 범위에 놓이지만, 이 범위보다 약간 높거나 낮은 값도 다소 존재하게 됩니다.</p>
</blockquote>

<h3 id="handling-extreme-outliers">
<a class="anchor" href="#handling-extreme-outliers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Handling extreme outliers (극단적 이상점 처리)</h3>

<p>roomsPerPerson = totalRooms / population 시 1 인당 방이 50 개 상태에서</p>

<p>매우 긴 꼬리 경우:</p>

<p>이러한 극단적 이상점이 주는 영향을 최소화하려면 어떻게 해야 할까요?</p>

<p>모든 값의 로그를 취하기: roomsPerPerson = log((totalRooms / population) + 1)</p>

<p>로그 조정을 거쳐도 꼬리가 남아 있는 경우:</p>

<p>로그 조정을 거치면 상황이 다소 개선되지만, 이상점 값의 꼬리가 아직도 상당히 남아 있습니다.<br>
다른 접근법을 시도해 보겠습니다.
roomsPerPerson의 최대값을 4.0 같은 임의의 지점에서 잘라내어 제한을 두면 어떻게 될까요?</p>

<p>roomsPerPerson = min(totalRooms / population, 4)</p>

<p>특성 값을 4.0에서 잘라낸다는 말은 4.0보다 큰 값을 모두 무시한다는 의미가 아니라, 4.0보다 큰 값을 모두 4.0으로 <strong>인식</strong>하겠다는 의미입니다.<br>
따라서 4.0 지점에 부자연스러운 경사가 생깁니다.<br>
하지만 조정된 특성 세트는 원래 데이터보다 훨씬 유용해진 상태입니다.</p>

<h3 id="binning">
<a class="anchor" href="#binning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Binning (비닝)</h3>

<p>다음은 캘리포니아의 위도에 따른 상대적인 주택 분포를 보여주는 플롯입니다.
클러스터링을 잘 살펴보세요.
로스앤젤레스의 위도는 약 34도이고 샌프란시스코의 위도는 약 38도입니다.</p>

<p>데이터 세트에서 latitude는 부동 소수점 값입니다.
그러나 이 모델에서 latitude를 부동 소수점 특성으로 표현할 수는 없는데, 이는 위도와 주택 값 사이에 선형적 관계가 없기 때문입니다.
예를 들어 위도 35도에 위치한 주택이 위도 34도에 위치한 주택보다 35/34만큼 싸거나 비싸지는 않습니다.
그러나 각각의 위도 값은 주택 가격을 예측하는 좋은 지표일 가능성이 높습니다.</p>

<p>위도를 유용한 예측 지표로 사용하기 위해 다음 그림과 같이 위도를 여러 '빈(bin)'으로 나누어 보겠습니다.</p>

<p><img src="%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EC%A0%95%EC%A0%9C_%EB%B9%84%EB%8B%9D.svg" alt="데이터 정제_비닝.svg"></p>

<p>이제 부동 소수점 특성 하나가 아니라 11개의 개별 부울 특성(LatitudeBin1, LatitudeBin2, ..., LatitudeBin11)이 생겼습니다.<br>
11개의 개별 특성은 다소 번잡하므로 하나의 11원소 벡터로 통일하겠습니다.  </p>

<p>이렇게 하면 위도 37.4도를 다음과 같이 표현할 수 있습니다:</p>
<div class="highlight"><pre><code class="language-py" data-lang="py"><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</code></pre></div>
<p>비닝을 사용하면 모델에서 각 위도에 대해 완전히 다른 가중치를 익힐 수 있습니다.</p>

<blockquote>
<p><strong>비닝 경계</strong></p>

<p>위도 예제에서는 이해하기 쉽도록 빈 경계로 정수를 사용했습니다.
해상도를 더 높여야 한다면 빈 경계를 1/10도 간격으로 나눌 수도 있습니다.
빈을 더 추가하면 모델에서 위도 37.4도와 37.5도에 대해 서로 다른 동작을 익힐 수 있지만, 1/10도 간격으로 충분한 수의 예가 확보되어야 한다는 전제가 붙습니다.</p>

<p><a href="https://wikipedia.org/wiki/Quantile">분위</a>를 기준으로 빈을 나누는 방법도 있는데, 이렇게 하면 각 버킷에 같은 개수의 예가 포함됩니다.
분위별 비닝을 사용하면 이상점에 대해 전혀 신경쓸 필요가 없습니다.</p>
</blockquote>

<h3 id="scrubbing">
<a class="anchor" href="#scrubbing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Scrubbing (스크러빙)</h3>

<p>실무에서 다음과 같은 이유는 데이터 세트의 여러 예를 신뢰할 수 없음:</p>

<ul>
<li>
<strong>값 누락:</strong> 사용자가 주택의 연령을 실수로 입력하지 않은 경우.</li>
<li>
<strong>중복 예.</strong> 서버에서 같은 로그를 실수로 두 번 업로드 한 경우.</li>
<li>
<strong>잘못된 라벨:</strong> 사용자가 참나무 사진에 실수로 단풍나무 라벨을 지정한 경우.</li>
<li>
<strong>잘못된 특성 값:</strong> 사용자가 숫자를 실수로 입력 또는 온도계를 햇빛에 둔 경우.</li>
</ul>

<p>잘못된 예가 발견되면 일반적으로 데이터 세트에서 삭제하여 해당 예를 '수정'합니다.<br>
값 누락이나 중복 예를 탐지하고자 간단한 프로그램을 작성할 수 있습니다.<br>
잘못된 특성 값 또는 라벨을 탐지하기는 훨씬 더 까다로울 수 있습니다.</p>

<p>잘못된 개별 예를 탐지하는 것 외에 집계에서도 잘못된 데이터를 탐지해야 합니다.<br>
히스토그램은 집계 데이터를 시각화하는 유용한 메커니즘입니다.<br>
또한 다음과 같은 통계를 구하면 도움이 될 수 있습니다.</p>

<ul>
<li>최대 및 최소</li>
<li>평균 및 중앙값</li>
<li>표준편차</li>
</ul>

<p>불연속 특성에서 가장 자주 나타나는 값으로 목록을 생성해보기.<br>
예를 들어 country:uk가 예상한 숫자와 일치하는 예의 개수를 세어 보세요.<br>
데이터 세트에서 language:jp를 가장 자주 나타나는 언어로 보아야 할까요?</p>

<h3 id="know-your-data">
<a class="anchor" href="#know-your-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Know your data (철저한 데이터 파악)</h3>

<p>다음과 같은 규칙을 따르세요.</p>

<ul>
<li>정상적인 데이터가 어떠한 모습이어야 하는지 항상 생각하기.</li>
<li>데이터가 이러한 예상과 일치하는지 확인하고, 그렇지 않다면 그 이유를 파악하기.</li>
<li>학습 데이터가 대시보드 등의 다른 소스와 일치하는지 재차 확인하기.</li>
</ul>

<p>핵심적인 코드를 다룰 때와 마찬가지로 데이터에 온 정성을 쏟아야 합니다.<br>
좋은 데이터가 좋은 ML을 만듭니다.</p>

<blockquote>
<p>Key Terms (주요 용어)</p>

<ul>
<li>
<a href="https://developers.google.com/machine-learning/crash-course/glossary?hl=ko#binning">비닝</a>: <a href="https://developers.google.com/machine-learning/crash-course/glossary?hl=ko#bucketing">버케팅</a> 참조.</li>
<li>
<a href="https://developers.google.com/machine-learning/crash-course/glossary?hl=ko#feature_set">특성 세트</a>: 머신러닝 모델에서 학습에 사용하는 특성 그룹입니다.</li>
<li>
<a href="https://developers.google.com/machine-learning/crash-course/glossary?hl=ko#NaN_trap">NaN 트랩</a>: 모델의 숫자 중 하나가 학습 중에 NaN이 됨으로 인해 모델의 다른 여러 숫자 또는 모든 숫자가 결국 NaN이 되는 상황입니다.</li>
<li>
<a href="https://developers.google.com/machine-learning/crash-course/glossary?hl=ko#outliers">이상점</a>: 다른 대부분의 값과 동떨어진 값입니다.</li>
<li>
<a href="https://developers.google.com/machine-learning/crash-course/glossary?hl=ko#scaling">조정</a>: 특성 추출에서 널리 사용되는 방식으로서 특성 값 범위를 데이터 세트의 다른 특성 범위와 일치하도록 맞춥니다.</li>
<li>
<a href="https://developers.google.com/machine-learning/crash-course/glossary?hl=ko#bucketing">버케팅</a>: 하나의 특성(일반적으로 연속)을 버킷(bucket) 또는 빈(bin)이라고 하는 여러 이진 특성으로 변환하는 작업으로서, 일반적으로 값 범위를 기준으로 합니다.</li>
</ul>
</blockquote>

  </div>

<div>
  <ul>
<li><a href="#scaling-feature-values-">Scaling feature values (특성 값 조정)</a></li>
<li><a href="#handling-extreme-outliers-">Handling extreme outliers (극단적 이상점 처리)</a></li>
<li><a href="#binning-">Binning (비닝)</a></li>
<li><a href="#scrubbing-">Scrubbing (스크러빙)</a></li>
<li><a href="#know-your-data-">Know your data (철저한 데이터 파악)</a></li>
</ul>

<p>한 그루의 사과나무에서는 좋은 사과와 벌레 먹은 사과가 같이 열립니다.
그러나 과일 가게에서 판매하는 과일들은 모두 최고의 품질을 자랑합니다.
유통 과정 중간에서 누군가가 심혈을 기울여 불량한 과일을 솎아내고 약간 흠집이 있는 과일을 깨끗이 손질한 것입니다.
ML 엔지니어 역시 불량한 예를 솎아내고 약간 문제가 있는 예를 깨끗이 손질하는 데 막대한 노력을 기울입니다.
'나쁜 사과' 몇 개가 거대한 데이터 세트를 망칠 수 있습니다.</p>

<h3 id="scaling-feature-values">Scaling feature values (특성 값 조정)</h3>

<p><strong>조정</strong>이란 부동 소수점 특성 값을 100~900 등의 자연 범위에서 0~1 또는 -1~+1 등의 표준 범위로 변환하는 작업입니다.</p>

<p>특성 세트가 단일 특성으로만 구성된 경우 조정에 따르는 실질적인 이점은 거의 없습니다.
그러나 특성 세트가 여러 특성으로 구성되었다면 특성 조정으로 다음과 같은 이점을 누릴 수 있습니다:</p>

<ul>
<li><p>경사하강법이 더 빠르게 수렴됩니다.</p></li>
<li><p>'NaN 트랩'이 방지됩니다.<br>
NaN 트랩이란 모델의 숫자 중 하나가 NaN(예: 학습 중에 값이 부동 소수점 정밀도 한도를 초과하는 경우)이 된 후 수학 연산 과정에서 모델의 다른 모든 숫자가 결국 NaN이 되는 상황입니다.</p></li>
<li><p>모델이 각 특성의 적절한 가중치를 익히는 데 도움이 됩니다.<br>
특성 조정을 수행하지 않으면 모델에서 범위가 더 넓은 특성을 과도하게 중시합니다.</p></li>
</ul>

<p>모든 부동 소수점 특성에 동일한 척도를 부여할 필요는 없습니다.
특성 A는 -1~+1로, 특성 B는 -3~+3으로 조정해도 심각한 부작용은 없습니다.
그러나 특성 B를 5000~100000으로 조정하면 모델이 부정적으로 반응할 것입니다.</p>

<blockquote>
<p><strong>조정</strong>에 대한 상세내용:</p>

<p>숫자 데이터를 조정하는 알기 쉬운 방법 중 하나는 [최소값, 최대값]을 [-1, +1] 등의 작은 척도로 선형 매핑하는 것입니다.</p>

<p>각 값의 Z 점수를 계산하는 조정 방식도 널리 사용됩니다.<br>
Z 점수는 표준편차를 기준으로 평균에서 벗어난 정도를 계산합니다. 다시 말하면 다음과 같습니다.</p>

<p>scaledvalue = (value - mean)/stddev.</p>

<p>아래와 같은 예를 들어 보겠습니다.</p>

<ul>
<li>평균 = 100</li>
<li>표준편차 = 20</li>
<li>원래 값 = 130</li>
</ul>

<p>결과는 다음과 같습니다.</p>

<p>scaled_value = (130 - 100) / 20
  scaled_value = 1.5</p>

<p>Z 점수로 조정하면 대부분의 조정 값이 -3~+3 범위에 놓이지만, 이 범위보다 약간 높거나 낮은 값도 다소 존재하게 됩니다.</p>
</blockquote>

<h3 id="handling-extreme-outliers">Handling extreme outliers (극단적 이상점 처리)</h3>

<p>roomsPerPerson = totalRooms / population 시 1 인당 방이 50 개 상태에서</p>

<p>매우 긴 꼬리 경우:</p>

<p>이러한 극단적 이상점이 주는 영향을 최소화하려면 어떻게 해야 할까요?</p>

<p>모든 값의 로그를 취하기: roomsPerPerson = log((totalRooms / population) + 1)</p>

<p>로그 조정을 거쳐도 꼬리가 남아 있는 경우:</p>

<p>로그 조정을 거치면 상황이 다소 개선되지만, 이상점 값의 꼬리가 아직도 상당히 남아 있습니다.<br>
다른 접근법을 시도해 보겠습니다.
roomsPerPerson의 최대값을 4.0 같은 임의의 지점에서 잘라내어 제한을 두면 어떻게 될까요?</p>

<p>roomsPerPerson = min(totalRooms / population, 4)</p>

<p>특성 값을 4.0에서 잘라낸다는 말은 4.0보다 큰 값을 모두 무시한다는 의미가 아니라, 4.0보다 큰 값을 모두 4.0으로 <strong>인식</strong>하겠다는 의미입니다.<br>
따라서 4.0 지점에 부자연스러운 경사가 생깁니다.<br>
하지만 조정된 특성 세트는 원래 데이터보다 훨씬 유용해진 상태입니다.</p>

<h3 id="binning">Binning (비닝)</h3>

<p>다음은 캘리포니아의 위도에 따른 상대적인 주택 분포를 보여주는 플롯입니다.
클러스터링을 잘 살펴보세요.
로스앤젤레스의 위도는 약 34도이고 샌프란시스코의 위도는 약 38도입니다.</p>

<p>데이터 세트에서 latitude는 부동 소수점 값입니다.
그러나 이 모델에서 latitude를 부동 소수점 특성으로 표현할 수는 없는데, 이는 위도와 주택 값 사이에 선형적 관계가 없기 때문입니다.
예를 들어 위도 35도에 위치한 주택이 위도 34도에 위치한 주택보다 35/34만큼 싸거나 비싸지는 않습니다.
그러나 각각의 위도 값은 주택 가격을 예측하는 좋은 지표일 가능성이 높습니다.</p>

<p>위도를 유용한 예측 지표로 사용하기 위해 다음 그림과 같이 위도를 여러 '빈(bin)'으로 나누어 보겠습니다.</p>

<p><img src="%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EC%A0%95%EC%A0%9C_%EB%B9%84%EB%8B%9D.svg" alt="데이터 정제_비닝.svg"></p>

<p>이제 부동 소수점 특성 하나가 아니라 11개의 개별 부울 특성(LatitudeBin1, LatitudeBin2, ..., LatitudeBin11)이 생겼습니다.<br>
11개의 개별 특성은 다소 번잡하므로 하나의 11원소 벡터로 통일하겠습니다.  </p>

<p>이렇게 하면 위도 37.4도를 다음과 같이 표현할 수 있습니다:</p>
<div class="highlight"><pre><code class="language-py" data-lang="py"><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</code></pre></div>
<p>비닝을 사용하면 모델에서 각 위도에 대해 완전히 다른 가중치를 익힐 수 있습니다.</p>

<blockquote>
<p><strong>비닝 경계</strong></p>

<p>위도 예제에서는 이해하기 쉽도록 빈 경계로 정수를 사용했습니다.
해상도를 더 높여야 한다면 빈 경계를 1/10도 간격으로 나눌 수도 있습니다.
빈을 더 추가하면 모델에서 위도 37.4도와 37.5도에 대해 서로 다른 동작을 익힐 수 있지만, 1/10도 간격으로 충분한 수의 예가 확보되어야 한다는 전제가 붙습니다.</p>

<p><a href="https://wikipedia.org/wiki/Quantile" rel="nofollow" target="_blank">분위</a>를 기준으로 빈을 나누는 방법도 있는데, 이렇게 하면 각 버킷에 같은 개수의 예가 포함됩니다.
분위별 비닝을 사용하면 이상점에 대해 전혀 신경쓸 필요가 없습니다.</p>
</blockquote>

<h3 id="scrubbing">Scrubbing (스크러빙)</h3>

<p>실무에서 다음과 같은 이유는 데이터 세트의 여러 예를 신뢰할 수 없음:</p>

<ul>
<li>
<strong>값 누락:</strong> 사용자가 주택의 연령을 실수로 입력하지 않은 경우.</li>
<li>
<strong>중복 예.</strong> 서버에서 같은 로그를 실수로 두 번 업로드 한 경우.</li>
<li>
<strong>잘못된 라벨:</strong> 사용자가 참나무 사진에 실수로 단풍나무 라벨을 지정한 경우.</li>
<li>
<strong>잘못된 특성 값:</strong> 사용자가 숫자를 실수로 입력 또는 온도계를 햇빛에 둔 경우.</li>
</ul>

<p>잘못된 예가 발견되면 일반적으로 데이터 세트에서 삭제하여 해당 예를 '수정'합니다.<br>
값 누락이나 중복 예를 탐지하고자 간단한 프로그램을 작성할 수 있습니다.<br>
잘못된 특성 값 또는 라벨을 탐지하기는 훨씬 더 까다로울 수 있습니다.</p>

<p>잘못된 개별 예를 탐지하는 것 외에 집계에서도 잘못된 데이터를 탐지해야 합니다.<br>
히스토그램은 집계 데이터를 시각화하는 유용한 메커니즘입니다.<br>
또한 다음과 같은 통계를 구하면 도움이 될 수 있습니다.</p>

<ul>
<li>최대 및 최소</li>
<li>평균 및 중앙값</li>
<li>표준편차</li>
</ul>

<p>불연속 특성에서 가장 자주 나타나는 값으로 목록을 생성해보기.<br>
예를 들어 country:uk가 예상한 숫자와 일치하는 예의 개수를 세어 보세요.<br>
데이터 세트에서 language:jp를 가장 자주 나타나는 언어로 보아야 할까요?</p>

<h3 id="know-your-data">Know your data (철저한 데이터 파악)</h3>

<p>다음과 같은 규칙을 따르세요.</p>

<ul>
<li>정상적인 데이터가 어떠한 모습이어야 하는지 항상 생각하기.</li>
<li>데이터가 이러한 예상과 일치하는지 확인하고, 그렇지 않다면 그 이유를 파악하기.</li>
<li>학습 데이터가 대시보드 등의 다른 소스와 일치하는지 재차 확인하기.</li>
</ul>

<p>핵심적인 코드를 다룰 때와 마찬가지로 데이터에 온 정성을 쏟아야 합니다.<br>
좋은 데이터가 좋은 ML을 만듭니다.</p>

<blockquote>
<p>Key Terms (주요 용어)</p>

<ul>
<li>
<a href="https://developers.google.com/machine-learning/crash-course/glossary?hl=ko#binning" rel="nofollow" target="_blank">비닝</a>: <a href="https://developers.google.com/machine-learning/crash-course/glossary?hl=ko#bucketing" rel="nofollow" target="_blank">버케팅</a> 참조.</li>
<li>
<a href="https://developers.google.com/machine-learning/crash-course/glossary?hl=ko#feature_set" rel="nofollow" target="_blank">특성 세트</a>: 머신러닝 모델에서 학습에 사용하는 특성 그룹입니다.</li>
<li>
<a href="https://developers.google.com/machine-learning/crash-course/glossary?hl=ko#NaN_trap" rel="nofollow" target="_blank">NaN 트랩</a>: 모델의 숫자 중 하나가 학습 중에 NaN이 됨으로 인해 모델의 다른 여러 숫자 또는 모든 숫자가 결국 NaN이 되는 상황입니다.</li>
<li>
<a href="https://developers.google.com/machine-learning/crash-course/glossary?hl=ko#outliers" rel="nofollow" target="_blank">이상점</a>: 다른 대부분의 값과 동떨어진 값입니다.</li>
<li>
<a href="https://developers.google.com/machine-learning/crash-course/glossary?hl=ko#scaling" rel="nofollow" target="_blank">조정</a>: 특성 추출에서 널리 사용되는 방식으로서 특성 값 범위를 데이터 세트의 다른 특성 범위와 일치하도록 맞춥니다.</li>
<li>
<a href="https://developers.google.com/machine-learning/crash-course/glossary?hl=ko#bucketing" rel="nofollow" target="_blank">버케팅</a>: 하나의 특성(일반적으로 연속)을 버킷(bucket) 또는 빈(bin)이라고 하는 여러 이진 특성으로 변환하는 작업으로서, 일반적으로 값 범위를 기준으로 합니다.</li>
</ul>
</blockquote>

</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Portal2312&#39;s blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Portal2312&#39;s blog</li><li><a class="u-email" href="mailto:portal2312@gmail.com">portal2312@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/portal2312"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">portal2312</span></a></li><li><a href="https://www.twitter.com/portal2312"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">portal2312</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Welcome to my blog.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
