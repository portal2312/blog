<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>L2 Regularization (L2 정규화) | Portal2312&#39;s blog</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="L2 Regularization (L2 정규화)" />
<meta name="author" content="mkkim" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Welcome to my blog." />
<meta property="og:description" content="Welcome to my blog." />
<link rel="canonical" href="/blog/docs/develop/A-M/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/011.%20Regularization:%20Simplicity%20(%EB%8B%A8%EC%88%9C%EC%84%B1%EC%9D%84%20%EC%9C%84%ED%95%9C%20%EC%A0%95%EA%B7%9C%ED%99%94,%2045%20min)/003.%20L2%20%EC%A0%95%EA%B7%9C%ED%99%94.html" />
<meta property="og:url" content="/blog/docs/develop/A-M/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/011.%20Regularization:%20Simplicity%20(%EB%8B%A8%EC%88%9C%EC%84%B1%EC%9D%84%20%EC%9C%84%ED%95%9C%20%EC%A0%95%EA%B7%9C%ED%99%94,%2045%20min)/003.%20L2%20%EC%A0%95%EA%B7%9C%ED%99%94.html" />
<meta property="og:site_name" content="Portal2312&#39;s blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-12-02T16:38:51+09:00" />
<script type="application/ld+json">
{"description":"Welcome to my blog.","headline":"L2 Regularization (L2 정규화)","dateModified":"2019-12-02T16:38:51+09:00","datePublished":"2019-12-02T16:38:51+09:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"/blog/docs/develop/A-M/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/011.%20Regularization:%20Simplicity%20(%EB%8B%A8%EC%88%9C%EC%84%B1%EC%9D%84%20%EC%9C%84%ED%95%9C%20%EC%A0%95%EA%B7%9C%ED%99%94,%2045%20min)/003.%20L2%20%EC%A0%95%EA%B7%9C%ED%99%94.html"},"url":"/blog/docs/develop/A-M/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/011.%20Regularization:%20Simplicity%20(%EB%8B%A8%EC%88%9C%EC%84%B1%EC%9D%84%20%EC%9C%84%ED%95%9C%20%EC%A0%95%EA%B7%9C%ED%99%94,%2045%20min)/003.%20L2%20%EC%A0%95%EA%B7%9C%ED%99%94.html","author":{"@type":"Person","name":"mkkim"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href='/blog/assets/main.css'><link type="application/atom+xml" rel="alternate" href="/blog/feed.xml" title="Portal2312's blog" /><script>
if(!(window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1")) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-113063601-1', 'auto');
  ga('send', 'pageview');
}
</script>
  
<script src='/blog/dist/js/common.bundle.js'></script>
</head>
<body><header class="site-header" role="banner">
  <div class="wrapper">
    <a class="site-title" rel="author" href="/blog/">Portal2312&#39;s blog</a>
    <nav class="site-nav">
    <input type="checkbox" id="nav-trigger" class="nav-trigger" />
    <label for="nav-trigger">
      <span class="menu-icon">
        <svg viewBox="0 0 18 15" width="18px" height="15px">
          <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
        </svg>
      </span>
    </label>

    <div class="trigger"><a class="page-link" href="/blog/about.html">
            About
          </a><a class="page-link" href="/blog/posts.html">
            Posts
          </a><a class="page-link" href="/blog/history.html">
            History
          </a><a class="page-link" href="/blog/docs/index.html">
            Docs
          </a></div>
  </nav>
  </div>
  <div class="scroll-indicator-container">
  <div class="scroll-indicator-bar" id="scrollIndicatorBar"></div>
</div>

</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <h1>L2 Regularization (L2 정규화)</h1>

  <div>
    <h2>Table of contents</h2>
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#reference">Reference</a></li>
</ul><p><img src="RegularizationTwoLossFunctions.svg" alt="RegularizationTwoLossFunctions.svg"></p>

<p>그림 1은 학습 손실은 점차 감소하지만 검증 손실은 결국 증가하는 모델을 보여줍니다.<br>
즉, 이 일반화 곡선은 모델이 학습 세트의 데이터에 대해 과적합하다는 것을 보여줍니다.</p>

<p>이전에 언급했던 Occam 개념을 활용하면 복잡한 모델에 페널티를 부여하는 정규화라는 원칙을 사용하여 과적합을 방지할 수 있을지도 모릅니다.</p>

<p>다시 말해 다음은 단순히 손실을 최소화하는 것만을 목표로 삼습니다 (경험적 위험 최소화):</p>
<div class="highlight"><pre><code class="language-" data-lang="">최소화(손실 (데이터|모델))
</code></pre></div>
<p><strong>구조적 위험 최소화</strong>를 통해 다음과 같이 손실과 복잡도를 함께 최소화:</p>
<div class="highlight"><pre><code class="language-" data-lang="">최소화(손실 (데이터|모델) + 복잡도(모델))
</code></pre></div>
<p>이제 우리의 학습 최적화 알고리즘은 다음과 같은 함수가 됩니다:
- 모델이 데이터에 얼마나 적합한지 측정하는 <strong>손실 항</strong>
- 모델 복잡도를 측정하는 <strong>정규화 항</strong></p>

<p>일반적인 (그리고 어느 정도 서로 관련이 있는) 2가지 방법으로 모델 복잡도를 다루게 됩니다.</p>

<ul>
<li>모델의 모든 특성의 가중치에 대한 함수로서의 모델 복잡도</li>
<li>0이 아닌 가중치를 사용하는 특성의 총 개수에 대한 함수로서의 모델 복잡도 (후속 강의에서 이 접근 방식을 다룸)</li>
</ul>

<p>모델 복잡도 = 가중치에 대한 함수인 경우 특성 가중치(weight) 절대값 높으면 모델 복잡도 높음.</p>

<p>모든 특성 가중치를 제곱한 값의 합계로서 정규화 항을 정의하는 <strong>L2 정규화</strong> 공식을 사용하여 복잡도를 수치화할 수 있습니다:</p>

<p>$$
L_2\text{정규화 항} = ||\boldsymbol w||_2^2 = {w_1^2 + w_2^2 + ... + w_n^2}
$$</p>

<p>이 공식에서 0에 가까운 가중치는 모델 복잡도에 거의 영향을 미치지 않는 반면, 이상점 가중치는 큰 영향을 미칠 수 있습니다.</p>

<p>예를 들어 다음과 같은 가중치를 갖는 선형 모델:</p>

<p>$$
{w_1 = 0.2, w_2 = 0.5, w_3 = 5, w_4 = 1, w_5 = 0.25, w_6 = 0.75}
$$</p>

<p>위 모델의 L2 정규화 항은 다음과 같이 26.915입니다:</p>

<p>$$
w_1^2 + w_2^2 + \boldsymbol{w_3^2} + w_4^2 + w_5^2 + w_6^2\
= 0.2^2 + 0.5^2 + \boldsymbol{5^2} + 1^2 + 0.25^2 + 0.75^2\
= 0.04 + 0.25 + \boldsymbol{25} + 1 + 0.0625 + 0.5625\
= 26.915
$$</p>

<p>하지만 제곱한 값이 25인 위의 굵은 글씨체로 $w_3$된 는 거의 모든 복잡도에 기여합니다.<br>
다른 5개의 모든 가중치를 제곱한 값의 합계는 $L_2$ 정규화 항에 1.915를 더하기만 하면 됩니다.</p>

<h2 id="reference">
<a class="anchor" href="#reference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reference</h2>

<p><a href="https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization?hl=ko">https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization?hl=ko</a></p>

  </div>

<div>
  <p><img src="RegularizationTwoLossFunctions.svg" alt="RegularizationTwoLossFunctions.svg"></p>

<p>그림 1은 학습 손실은 점차 감소하지만 검증 손실은 결국 증가하는 모델을 보여줍니다.<br>
즉, 이 일반화 곡선은 모델이 학습 세트의 데이터에 대해 과적합하다는 것을 보여줍니다.</p>

<p>이전에 언급했던 Occam 개념을 활용하면 복잡한 모델에 페널티를 부여하는 정규화라는 원칙을 사용하여 과적합을 방지할 수 있을지도 모릅니다.</p>

<p>다시 말해 다음은 단순히 손실을 최소화하는 것만을 목표로 삼습니다 (경험적 위험 최소화):</p>
<div class="highlight"><pre><code class="language-" data-lang="">최소화(손실 (데이터|모델))
</code></pre></div>
<p><strong>구조적 위험 최소화</strong>를 통해 다음과 같이 손실과 복잡도를 함께 최소화:</p>
<div class="highlight"><pre><code class="language-" data-lang="">최소화(손실 (데이터|모델) + 복잡도(모델))
</code></pre></div>
<p>이제 우리의 학습 최적화 알고리즘은 다음과 같은 함수가 됩니다:
- 모델이 데이터에 얼마나 적합한지 측정하는 <strong>손실 항</strong>
- 모델 복잡도를 측정하는 <strong>정규화 항</strong></p>

<p>일반적인 (그리고 어느 정도 서로 관련이 있는) 2가지 방법으로 모델 복잡도를 다루게 됩니다.</p>

<ul>
<li>모델의 모든 특성의 가중치에 대한 함수로서의 모델 복잡도</li>
<li>0이 아닌 가중치를 사용하는 특성의 총 개수에 대한 함수로서의 모델 복잡도 (후속 강의에서 이 접근 방식을 다룸)</li>
</ul>

<p>모델 복잡도 = 가중치에 대한 함수인 경우 특성 가중치(weight) 절대값 높으면 모델 복잡도 높음.</p>

<p>모든 특성 가중치를 제곱한 값의 합계로서 정규화 항을 정의하는 <strong>L2 정규화</strong> 공식을 사용하여 복잡도를 수치화할 수 있습니다:</p>

<p>$$
L_2\text{정규화 항} = ||\boldsymbol w||_2^2 = {w_1^2 + w_2^2 + ... + w_n^2}
$$</p>

<p>이 공식에서 0에 가까운 가중치는 모델 복잡도에 거의 영향을 미치지 않는 반면, 이상점 가중치는 큰 영향을 미칠 수 있습니다.</p>

<p>예를 들어 다음과 같은 가중치를 갖는 선형 모델:</p>

<p>$$
{w_1 = 0.2, w_2 = 0.5, w_3 = 5, w_4 = 1, w_5 = 0.25, w_6 = 0.75}
$$</p>

<p>위 모델의 L2 정규화 항은 다음과 같이 26.915입니다:</p>

<p>$$
w_1^2 + w_2^2 + \boldsymbol{w_3^2} + w_4^2 + w_5^2 + w_6^2\
= 0.2^2 + 0.5^2 + \boldsymbol{5^2} + 1^2 + 0.25^2 + 0.75^2\
= 0.04 + 0.25 + \boldsymbol{25} + 1 + 0.0625 + 0.5625\
= 26.915
$$</p>

<p>하지만 제곱한 값이 25인 위의 굵은 글씨체로 $w_3$된 는 거의 모든 복잡도에 기여합니다.<br>
다른 5개의 모든 가중치를 제곱한 값의 합계는 $L_2$ 정규화 항에 1.915를 더하기만 하면 됩니다.</p>

<h2 id="reference">Reference</h2>

<p><a href="https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization?hl=ko" rel="nofollow" target="_blank">https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization?hl=ko</a></p>

</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Portal2312&#39;s blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Portal2312&#39;s blog</li><li><a class="u-email" href="mailto:portal2312@gmail.com">portal2312@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/portal2312"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">portal2312</span></a></li><li><a href="https://www.twitter.com/portal2312"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">portal2312</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Welcome to my blog.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
