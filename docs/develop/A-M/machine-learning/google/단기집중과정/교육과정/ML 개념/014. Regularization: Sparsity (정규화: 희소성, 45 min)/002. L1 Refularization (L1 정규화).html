<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Regularization for Sparsity - L₁ Regularization (희소성을 위한 정규화 - L₁ 정규화) | Portal2312&#39;s blog</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="Regularization for Sparsity - L₁ Regularization (희소성을 위한 정규화 - L₁ 정규화)" />
<meta name="author" content="mkkim" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="희소 벡터는 종종 많은 차원을 포함합니다. 특성 교차를 생성하면 더 많은 차원이 발생합니다. 이러한 고차원 특성 벡터가 주어지면 모델 크기가 커질 수 있으며 엄청난 양의 RAM이 필요합니다. 가능하다면 고차원의 희소 벡터에서는 가중치가 정확하게 0 으로 떨어지도록 유도하기." />
<meta property="og:description" content="희소 벡터는 종종 많은 차원을 포함합니다. 특성 교차를 생성하면 더 많은 차원이 발생합니다. 이러한 고차원 특성 벡터가 주어지면 모델 크기가 커질 수 있으며 엄청난 양의 RAM이 필요합니다. 가능하다면 고차원의 희소 벡터에서는 가중치가 정확하게 0 으로 떨어지도록 유도하기." />
<link rel="canonical" href="/blog/docs/develop/A-M/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/014.%20Regularization:%20Sparsity%20(%EC%A0%95%EA%B7%9C%ED%99%94:%20%ED%9D%AC%EC%86%8C%EC%84%B1,%2045%20min)/002.%20L1%20Refularization%20(L1%20%EC%A0%95%EA%B7%9C%ED%99%94).html" />
<meta property="og:url" content="/blog/docs/develop/A-M/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/014.%20Regularization:%20Sparsity%20(%EC%A0%95%EA%B7%9C%ED%99%94:%20%ED%9D%AC%EC%86%8C%EC%84%B1,%2045%20min)/002.%20L1%20Refularization%20(L1%20%EC%A0%95%EA%B7%9C%ED%99%94).html" />
<meta property="og:site_name" content="Portal2312&#39;s blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-12-02T16:38:51+09:00" />
<script type="application/ld+json">
{"description":"희소 벡터는 종종 많은 차원을 포함합니다. 특성 교차를 생성하면 더 많은 차원이 발생합니다. 이러한 고차원 특성 벡터가 주어지면 모델 크기가 커질 수 있으며 엄청난 양의 RAM이 필요합니다. 가능하다면 고차원의 희소 벡터에서는 가중치가 정확하게 0 으로 떨어지도록 유도하기.","headline":"Regularization for Sparsity - L₁ Regularization (희소성을 위한 정규화 - L₁ 정규화)","dateModified":"2019-12-02T16:38:51+09:00","datePublished":"2019-12-02T16:38:51+09:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"/blog/docs/develop/A-M/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/014.%20Regularization:%20Sparsity%20(%EC%A0%95%EA%B7%9C%ED%99%94:%20%ED%9D%AC%EC%86%8C%EC%84%B1,%2045%20min)/002.%20L1%20Refularization%20(L1%20%EC%A0%95%EA%B7%9C%ED%99%94).html"},"url":"/blog/docs/develop/A-M/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/014.%20Regularization:%20Sparsity%20(%EC%A0%95%EA%B7%9C%ED%99%94:%20%ED%9D%AC%EC%86%8C%EC%84%B1,%2045%20min)/002.%20L1%20Refularization%20(L1%20%EC%A0%95%EA%B7%9C%ED%99%94).html","author":{"@type":"Person","name":"mkkim"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href='/blog/assets/main.css'><link type="application/atom+xml" rel="alternate" href="/blog/feed.xml" title="Portal2312's blog" /><script>
if(!(window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1")) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-113063601-1', 'auto');
  ga('send', 'pageview');
}
</script>
  
<script src='/blog/dist/js/common.bundle.js'></script>
</head>
<body><header class="site-header" role="banner">
  <div class="wrapper">
    <a class="site-title" rel="author" href="/blog/">Portal2312&#39;s blog</a>
    <nav class="site-nav">
    <input type="checkbox" id="nav-trigger" class="nav-trigger" />
    <label for="nav-trigger">
      <span class="menu-icon">
        <svg viewBox="0 0 18 15" width="18px" height="15px">
          <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
        </svg>
      </span>
    </label>

    <div class="trigger"><a class="page-link" href="/blog/about.html">
            About
          </a><a class="page-link" href="/blog/posts.html">
            Posts
          </a><a class="page-link" href="/blog/history.html">
            History
          </a><a class="page-link" href="/blog/docs/index.html">
            Docs
          </a></div>
  </nav>
  </div>
  <div class="scroll-indicator-container">
  <div class="scroll-indicator-bar" id="scrollIndicatorBar"></div>
</div>

</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <h1>Regularization for Sparsity - L₁ Regularization (희소성을 위한 정규화 - L₁ 정규화)</h1>

  <div>
    <h2>Table of contents</h2>
    <ul class="section-nav">
<li class="toc-entry toc-h3"><a href="#l1-l2">L1 정규화와 L2 정규화 비교</a></li>
<li class="toc-entry toc-h2"><a href="#reference">Reference</a></li>
<li class="toc-entry toc-h2"><a href="#terms">Terms</a></li>
</ul><p>희소 벡터는 종종 많은 차원을 포함합니다.
<a href="https://developers.google.com/machine-learning/glossary/?hl=ko#feature_cross">특성 교차</a>를 생성하면 더 많은 차원이 발생합니다.
이러한 고차원 특성 벡터가 주어지면 모델 크기가 커질 수 있으며 엄청난 양의 RAM이 필요합니다.
가능하다면 고차원의 희소 벡터에서는 가중치가 정확하게 0 으로 떨어지도록 유도하기.</p>

<p>가중치가 정확하게 0일 경우 모델에서 해당 특성 삭제하기.
- RAM 절약
- 모델 노이즈 감소</p>

<p>예:</p>

<p>캘리포니아뿐만 아니라 전 세계를 포괄하는 주택 데이터 세트.</p>

<p>분 단위(1분 =1/60도)로 전 세계 위도를 버케팅하면 약 10,000개의 차원을, 전 세계 경도를 버케팅하면 약 20,000개의 차원을 희소 인코딩에서 사용할 수 있습니다.
이러한 두 가지 특성에 대한 특성 교차를 통해 약 2억 개의 차원이 생성됩니다.
2억 개의 차원 중 많은 부분이 제한된 거주 지역(예: 바다 한가운데)을 나타내므로 이러한 데이터를 효과적인 일반화를 위해 사용하기는 어려울 것입니다.</p>

<p>이와 같이 불필요한 차원을 저장하기 위해 RAM을 낭비하는 것은 어리석은 일일 것입니다.
따라서 무의미한 차원의 가중치가 정확히 0이 되도록 하는 것이 중요합니다.
그러면 추론 단계에서 이러한 모델 계수에 대한 저장 비용을 지불하지 않아도 됩니다.
적절히 선택한 정규화 항을 추가함으로써, 학습 시 수행한 최적화 문제에 이 아이디어를 적용할 수 있습니다.</p>

<p><strong>L2 정규화</strong>를 통해 목표를 달성할 수 있을까요? 안타깝게도 그렇지 않습니다.
가중치를 작은 값으로 유도하지만 정확히 0.0으로 만들지는 못하기 때문.</p>

<p>이에 대한 대안은 <strong>모델에서 0이 아닌 계수 값의 수에 페널티를 주는 정규화 항</strong>을 생성하기.
0 이 아닌 계수 값의 수를 늘리는 것이 정당화되는 유일한 경우는 <strong>모델의 데이터 적합성을 충분히 확보한 경우</strong> 입니다.</p>

<p>하지만 이 개수 기반 접근 방식은 직관적으로는 매력적이긴 하지만<br>
볼록 최적화 문제를 <a href="https://ko.wikipedia.org/wiki/NP-%EB%82%9C%ED%95%B4">NP-난해</a>인 볼록하지 않은 최적화 문제로 바꿔버리는 단점이 있습니다.</p>

<p>자세히 살펴보면 <a href="https://ko.wikipedia.org/wiki/%EB%B0%B0%EB%82%AD_%EB%AC%B8%EC%A0%9C">배낭 문제</a>와 관련이 있다는 것을 알 수 있습니다.
그렇기 때문에, <strong>L0 정규화</strong>로 알려진 이 아이디어를 실제로 효과적으로 사용할 수는 없습니다.</p>

<p>하지만 L0에 가까우면서도 볼록하다는 이점이 있어 계산하기에 효율적인 <strong>L1 정규화</strong>라는 정규화 항이 있습니다.<br>
L1 정규화를 사용하여 모델에서 유용하지 않은 많은 계수를 정확히 0이 되도록 유도하여 추론 단계에서 RAM을 절약할 수 있습니다.</p>

<h3 id="l1-l2">
<a class="anchor" href="#l1-l2" aria-hidden="true"><span class="octicon octicon-link"></span></a>L1 정규화와 L2 정규화 비교</h3>

<p>L2와 L1은 서로 다른 방식으로 가중치에 페널티:</p>

<ul>
<li>L2: 가중치2 에 페널티를 줍니다.</li>
<li>L1: |가중치| 에 페널티를 줍니다.</li>
</ul>

<p>결과적으로 L2와 L1은 서로 다르게 미분:</p>

<ul>
<li>L2의 미분계수는 2 * 가중치</li>
<li>L1의 미분계수는 k(가중치와 무관한 값을 갖는 상수)</li>
</ul>

<p>L2의 미분계수는 매번 가중치의 x%만큼 제거한다고 생각하면 됩니다.
어떤 수를 x%만큼 무한히 제거해도 그 값은 절대 0이 되지 않아 L2는 일반적으로 가중치를 0으로 유도하지 않습니다.</p>

<p>L1 의 미분계수는 매번 가중치에서 일정 상수를 빼는 것으로 생각하면 됩니다.
하지만 절대값으로 인해 L1은 0에서 불연속성을 가지며, 이로 인해 0을 지나는 빼기 결과값은 0이 되어 제거됩니다.</p>

<p>예:</p>

<p>빼기 연산으로 인해 가중치가 +0.1에서 -0.2이 된다면 L1은 가중치를 정확히 0으로 만들 수 있습니다. 드디어 발견했습니다. L1을 통해 가중치가 제거되었습니다.</p>

<p>모든 가중치의 절대값에 페널티를 주는 L1 정규화는 다양한 모델에서 아주 효율적으로 활용 가능.</p>

<p>이 설명은 1차원 모델에만 적용되는 점에 유의하세요.</p>

<h2 id="reference">
<a class="anchor" href="#reference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reference</h2>

<p><a href="https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization?hl=ko">https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization?hl=ko</a></p>

<h2 id="terms">
<a class="anchor" href="#terms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Terms</h2>

<blockquote>
<p><strong>특성 교차(feature cross)</strong><br>
범주형 데이터나 버케팅을 통한 연속 특성으로 부터 얻어진 개별 이진 특성들을 (데카르트 곱을 취해) 교차해 얻은 합성 특성입니다.
특성 교차는 비선형 관계를 표현하는 데 도움이 됩니다.</p>
</blockquote>

  </div>

<div>
  <p>희소 벡터는 종종 많은 차원을 포함합니다.
<a href="https://developers.google.com/machine-learning/glossary/?hl=ko#feature_cross" rel="nofollow" target="_blank">특성 교차</a>를 생성하면 더 많은 차원이 발생합니다.
이러한 고차원 특성 벡터가 주어지면 모델 크기가 커질 수 있으며 엄청난 양의 RAM이 필요합니다.
가능하다면 고차원의 희소 벡터에서는 가중치가 정확하게 0 으로 떨어지도록 유도하기.</p>

<p>가중치가 정확하게 0일 경우 모델에서 해당 특성 삭제하기.
- RAM 절약
- 모델 노이즈 감소</p>

<p>예:</p>

<p>캘리포니아뿐만 아니라 전 세계를 포괄하는 주택 데이터 세트.</p>

<p>분 단위(1분 =1/60도)로 전 세계 위도를 버케팅하면 약 10,000개의 차원을, 전 세계 경도를 버케팅하면 약 20,000개의 차원을 희소 인코딩에서 사용할 수 있습니다.
이러한 두 가지 특성에 대한 특성 교차를 통해 약 2억 개의 차원이 생성됩니다.
2억 개의 차원 중 많은 부분이 제한된 거주 지역(예: 바다 한가운데)을 나타내므로 이러한 데이터를 효과적인 일반화를 위해 사용하기는 어려울 것입니다.</p>

<p>이와 같이 불필요한 차원을 저장하기 위해 RAM을 낭비하는 것은 어리석은 일일 것입니다.
따라서 무의미한 차원의 가중치가 정확히 0이 되도록 하는 것이 중요합니다.
그러면 추론 단계에서 이러한 모델 계수에 대한 저장 비용을 지불하지 않아도 됩니다.
적절히 선택한 정규화 항을 추가함으로써, 학습 시 수행한 최적화 문제에 이 아이디어를 적용할 수 있습니다.</p>

<p><strong>L2 정규화</strong>를 통해 목표를 달성할 수 있을까요? 안타깝게도 그렇지 않습니다.
가중치를 작은 값으로 유도하지만 정확히 0.0으로 만들지는 못하기 때문.</p>

<p>이에 대한 대안은 <strong>모델에서 0이 아닌 계수 값의 수에 페널티를 주는 정규화 항</strong>을 생성하기.
0 이 아닌 계수 값의 수를 늘리는 것이 정당화되는 유일한 경우는 <strong>모델의 데이터 적합성을 충분히 확보한 경우</strong> 입니다.</p>

<p>하지만 이 개수 기반 접근 방식은 직관적으로는 매력적이긴 하지만<br>
볼록 최적화 문제를 <a href="https://ko.wikipedia.org/wiki/NP-%EB%82%9C%ED%95%B4" rel="nofollow" target="_blank">NP-난해</a>인 볼록하지 않은 최적화 문제로 바꿔버리는 단점이 있습니다.</p>

<p>자세히 살펴보면 <a href="https://ko.wikipedia.org/wiki/%EB%B0%B0%EB%82%AD_%EB%AC%B8%EC%A0%9C" rel="nofollow" target="_blank">배낭 문제</a>와 관련이 있다는 것을 알 수 있습니다.
그렇기 때문에, <strong>L0 정규화</strong>로 알려진 이 아이디어를 실제로 효과적으로 사용할 수는 없습니다.</p>

<p>하지만 L0에 가까우면서도 볼록하다는 이점이 있어 계산하기에 효율적인 <strong>L1 정규화</strong>라는 정규화 항이 있습니다.<br>
L1 정규화를 사용하여 모델에서 유용하지 않은 많은 계수를 정확히 0이 되도록 유도하여 추론 단계에서 RAM을 절약할 수 있습니다.</p>

<h3 id="l1-l2">L1 정규화와 L2 정규화 비교</h3>

<p>L2와 L1은 서로 다른 방식으로 가중치에 페널티:</p>

<ul>
<li>L2: 가중치2 에 페널티를 줍니다.</li>
<li>L1: |가중치| 에 페널티를 줍니다.</li>
</ul>

<p>결과적으로 L2와 L1은 서로 다르게 미분:</p>

<ul>
<li>L2의 미분계수는 2 * 가중치</li>
<li>L1의 미분계수는 k(가중치와 무관한 값을 갖는 상수)</li>
</ul>

<p>L2의 미분계수는 매번 가중치의 x%만큼 제거한다고 생각하면 됩니다.
어떤 수를 x%만큼 무한히 제거해도 그 값은 절대 0이 되지 않아 L2는 일반적으로 가중치를 0으로 유도하지 않습니다.</p>

<p>L1 의 미분계수는 매번 가중치에서 일정 상수를 빼는 것으로 생각하면 됩니다.
하지만 절대값으로 인해 L1은 0에서 불연속성을 가지며, 이로 인해 0을 지나는 빼기 결과값은 0이 되어 제거됩니다.</p>

<p>예:</p>

<p>빼기 연산으로 인해 가중치가 +0.1에서 -0.2이 된다면 L1은 가중치를 정확히 0으로 만들 수 있습니다. 드디어 발견했습니다. L1을 통해 가중치가 제거되었습니다.</p>

<p>모든 가중치의 절대값에 페널티를 주는 L1 정규화는 다양한 모델에서 아주 효율적으로 활용 가능.</p>

<p>이 설명은 1차원 모델에만 적용되는 점에 유의하세요.</p>

<h2 id="reference">Reference</h2>

<p><a href="https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization?hl=ko" rel="nofollow" target="_blank">https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization?hl=ko</a></p>

<h2 id="terms">Terms</h2>

<blockquote>
<p><strong>특성 교차(feature cross)</strong><br>
범주형 데이터나 버케팅을 통한 연속 특성으로 부터 얻어진 개별 이진 특성들을 (데카르트 곱을 취해) 교차해 얻은 합성 특성입니다.
특성 교차는 비선형 관계를 표현하는 데 도움이 됩니다.</p>
</blockquote>

</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Portal2312&#39;s blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Portal2312&#39;s blog</li><li><a class="u-email" href="mailto:portal2312@gmail.com">portal2312@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/portal2312"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">portal2312</span></a></li><li><a href="https://www.twitter.com/portal2312"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">portal2312</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Welcome to my blog.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
