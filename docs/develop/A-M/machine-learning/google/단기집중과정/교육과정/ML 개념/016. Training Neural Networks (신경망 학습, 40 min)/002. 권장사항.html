<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Base Practices (신경망 학습 - 권장사항) | Portal2312&#39;s blog</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="Base Practices (신경망 학습 - 권장사항)" />
<meta name="author" content="mkkim" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="이 섹션에서는 역전파의 실패 사례 및 신경망을 정규화하는 가장 일반적인 방법을 설명합니다." />
<meta property="og:description" content="이 섹션에서는 역전파의 실패 사례 및 신경망을 정규화하는 가장 일반적인 방법을 설명합니다." />
<link rel="canonical" href="/blog/docs/develop/A-M/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/016.%20Training%20Neural%20Networks%20(%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%ED%95%99%EC%8A%B5,%2040%20min)/002.%20%EA%B6%8C%EC%9E%A5%EC%82%AC%ED%95%AD.html" />
<meta property="og:url" content="/blog/docs/develop/A-M/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/016.%20Training%20Neural%20Networks%20(%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%ED%95%99%EC%8A%B5,%2040%20min)/002.%20%EA%B6%8C%EC%9E%A5%EC%82%AC%ED%95%AD.html" />
<meta property="og:site_name" content="Portal2312&#39;s blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-12-02T16:38:51+09:00" />
<script type="application/ld+json">
{"description":"이 섹션에서는 역전파의 실패 사례 및 신경망을 정규화하는 가장 일반적인 방법을 설명합니다.","headline":"Base Practices (신경망 학습 - 권장사항)","dateModified":"2019-12-02T16:38:51+09:00","datePublished":"2019-12-02T16:38:51+09:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"/blog/docs/develop/A-M/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/016.%20Training%20Neural%20Networks%20(%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%ED%95%99%EC%8A%B5,%2040%20min)/002.%20%EA%B6%8C%EC%9E%A5%EC%82%AC%ED%95%AD.html"},"url":"/blog/docs/develop/A-M/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/016.%20Training%20Neural%20Networks%20(%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%ED%95%99%EC%8A%B5,%2040%20min)/002.%20%EA%B6%8C%EC%9E%A5%EC%82%AC%ED%95%AD.html","author":{"@type":"Person","name":"mkkim"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href='/blog/assets/main.css'><link type="application/atom+xml" rel="alternate" href="/blog/feed.xml" title="Portal2312's blog" /><script>
if(!(window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1")) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-113063601-1', 'auto');
  ga('send', 'pageview');
}
</script>
  
<script src='/blog/dist/js/common.bundle.js'></script>
</head>
<body><header class="site-header" role="banner">
  <div class="wrapper">
    <a class="site-title" rel="author" href="/blog/">Portal2312&#39;s blog</a>
    <nav class="site-nav">
    <input type="checkbox" id="nav-trigger" class="nav-trigger" />
    <label for="nav-trigger">
      <span class="menu-icon">
        <svg viewBox="0 0 18 15" width="18px" height="15px">
          <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
        </svg>
      </span>
    </label>

    <div class="trigger"><a class="page-link" href="/blog/about.html">
            About
          </a><a class="page-link" href="/blog/posts.html">
            Posts
          </a><a class="page-link" href="/blog/history.html">
            History
          </a><a class="page-link" href="/blog/docs/index.html">
            Docs
          </a></div>
  </nav>
  </div>
  <div class="scroll-indicator-container">
  <div class="scroll-indicator-bar" id="scrollIndicatorBar"></div>
</div>

</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <h1>Base Practices (신경망 학습 - 권장사항)</h1>

  <div>
    <h2>Table of contents</h2>
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#part-41eb7c0ee8859147">실패 사례</a>
<ul>
<li class="toc-entry toc-h3"><a href="#part-2f832346e53f9b31">경사 소실</a></li>
<li class="toc-entry toc-h3"><a href="#part-2f832346e5eb3cad">경사 발산</a></li>
<li class="toc-entry toc-h3"><a href="#relu">ReLU 유닛 소멸</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#part-a242d157453f1c7f">드롭아웃 정규화</a></li>
<li class="toc-entry toc-h2"><a href="#reference">Reference</a></li>
</ul><p>이 섹션에서는 역전파의 실패 사례 및 신경망을 정규화하는 가장 일반적인 방법을 설명합니다.</p>

<h2 id="part-41eb7c0ee8859147">
<a class="anchor" href="#part-41eb7c0ee8859147" aria-hidden="true"><span class="octicon octicon-link"></span></a>실패 사례</h2>

<p>몇 가지 일반적인 이유로 인해 역전파에서 문제가 나타날 수 있습니다.</p>

<h3 id="part-2f832346e53f9b31">
<a class="anchor" href="#part-2f832346e53f9b31" aria-hidden="true"><span class="octicon octicon-link"></span></a>경사 소실</h3>

<p>입력 쪽에 가까운 하위 레이어의 경사가 매우 작아질 수 있습니다.</p>

<p>심층 네트워크에서 이러한 경사를 계산할 때는 많은 작은 항의 곱을 구하는 과정이 포함될 수 있습니다.</p>

<p>하위 레이어의 경사가 <em>0</em> 에 가깝게 소실되면 이러한 레이어에서 <strong>학습 속도가 크게 저하되거나 학습이 중지</strong>됩니다.</p>

<p><code>ReLU</code> 활성화 함수를 통해 경사 소실을 방지할 수 있습니다.</p>

<h3 id="part-2f832346e5eb3cad">
<a class="anchor" href="#part-2f832346e5eb3cad" aria-hidden="true"><span class="octicon octicon-link"></span></a>경사 발산</h3>

<p>네트워크에서 가중치(Weight)가 매우 크면 하위 레이어의 경사에 많은 큰 항의 곱이 포함됩니다.</p>

<p>이러한 경우 경사가 너무 커져서 수렴하지 못하고 발산하는 현상이 나타날 수 있습니다.</p>

<p><code>batch 정규화</code>를 사용하거나 학습률을 낮추면 경사 발산을 방지할 수 있습니다.</p>

<h3 id="relu">
<a class="anchor" href="#relu" aria-hidden="true"><span class="octicon octicon-link"></span></a>ReLU 유닛 소멸</h3>

<p><code>ReLU</code> 유닛의 가중 합이 <em>0</em> 미만으로 떨어지면 <code>ReLU</code> 유닛이 고착될 수 있습니다.</p>

<p>이러한 경우 활동이 출력되지 않으므로 네트워크의 출력에 어떠한 영향도 없으며 역전파 과정에서 경사가 더 이상 통과할 수 없습니다.<br>
이와 같이 경사의 근원이 단절되므로, 가중 합이 다시 <em>0</em> 이상으로 상승할 만큼 <code>ReLU</code>가 변화하지 못할 수도 있습니다.</p>

<p>학습률(Learning rate)을 낮추면 <code>ReLU</code> 유닛 소멸을 방지할 수 있습니다.</p>

<h2 id="part-a242d157453f1c7f">
<a class="anchor" href="#part-a242d157453f1c7f" aria-hidden="true"><span class="octicon octicon-link"></span></a>드롭아웃 정규화</h2>

<ul>
<li>또 하나의 정규화 형태, NN 에 유용</li>
<li>신경망에는 <strong>드롭아웃</strong>이라는 또 하나의 정규화 형태가 유용</li>
<li>이는 단일 경사 스텝에서 유닛 활동을 무작위로 배제하는 방식

<ul>
<li>앙상블 모델과의 접점</li>
</ul>
</li>
<li>드롭아웃을 반복할수록 정규화가 강력짐

<ul>
<li>0.0 = 드롭아웃 정규화 없음</li>
<li>1.0 = 전체 드롭아웃. 모델에서 학습을 수행 안함</li>
<li>0.0~1.0 범위 값 = 중간 범위의 값이 유용함</li>
</ul>
</li>
<li>그롭아웃으로 심층 신경망 (Deep learning) 이 주목받게 됨.</li>
</ul>

<blockquote>
<p><strong>주요 용어:</strong>
- <a href="https://developers.google.com/machine-learning/crash-course/glossary#activation_function">활성화 함수</a>:<br>
이전 레이어의 모든 입력에 대한 가중 합을 취하고 출력 값(일반적으로 비선형)을 생성하여 다음 레이어로 전달하는 <code>ReLU</code>, <code>시그모이드</code> 등의 함수입니다.
- <a href="https://developers.google.com/machine-learning/crash-course/glossary#backpropagation">역전파</a>:<br>
  - 신경망에서 <strong>경사하강법</strong> 을 수행하는 기본 알고리즘.
  - 우선, 정방향 단계에서 각 노드의 출력 값을 계산하고 캐시.
  - 그런 다음 역방향 단계에서 그래프를 통과하며 각 매개변수를 기준으로 오차의 편미분을 계산.
- <a href="https://developers.google.com/machine-learning/crash-course/glossary#dropout_regularization">드롭아웃 정규화</a>:
  - 신경망을 학습시키는 데 유용한 정규화 형태.
  - 사용시 단일 경사 스텝이 일어날 때마다 특정 네트워크 레이어의 유닛을 고정된 개수만큼 무작위로 선택하여 삭제.
  - 드롭아웃하는 유닛이 많을수록 정규화가 강력해짐.
  - 이 방식은 네트워크를 학습시켜 더 작은 네트워크로 이루어진 대규모 <strong>앙상블 기법(Ensemble)</strong> 을 모방하도록 하는 방식과 비슷합니다.
  - 자세한 내용은 드롭아웃: 신경망의 과적합을 방지하는 간단한 방법을 참조하세요.
    - 신경망 모델이 복잡해질 때 가중치 감소만으로는 어려운데 드롭아웃 기법은 뉴런의 연결을 임의로 삭제하는 것.
    - 훈련시 임의의 뉴런을 골라 삭제, 신호를 전달하지 않게 하기.
    - 테스트시 모든 뉴런을 사용.
- <a href="https://developers.google.com/machine-learning/crash-course/glossary#gradient_descent">경사하강법</a>:<br>
  - 학습 데이터의 조건에 따라 모델의 매개변수를 기준으로 손실의 경사를 계산하여 손실을 최소화하는 기법.
  - 쉽게 설명하면, 경사하강법은 매개변수를 반복적으로 조정하면서 손실을 최소화하는 가중치와 편향의 가장 적절한 조합을 점진적으로 찾는 방식.
- <a href="https://developers.google.com/machine-learning/crash-course/glossary#ReLU">정류 선형 유닛</a>(ReLU): 다음 규칙을 따르는 활성화 함수입니다.
  - 입력: 음수 또는 0 -&gt; 출력: 0
  - 입력: 양수 -&gt; 출력: 입력과 같음. </p>

<p><strong>가중치 감소(Weight Decay):</strong><br>
학습 중에 가중치가 큰 것에 대해서는 일종의 패널티를 부과해 과적합의 위험을 줄이는 방법이다.
가중치의 제곱 법칙(L2 법칙; 많이 사용된다)를 손실함수에 더해 손실함수 값이 더 커지게 한다.
그만큼 가중치가 커지는 것을 억제하기 되는 것이다.</p>

<p><strong>앙상블 기법(Ensemble):</strong>
서로 다른 모델들을 학습해서 개별 모델들에서 나온 출력의 평균을 내어 추론하는 학습 방식.
드롭아웃은 학습할 때 뉴런을 무작위로 학습해 매번 다른 모델들을 학습시킨다는 측면에서 앙상블 기법과 유사.</p>
</blockquote>

<h2 id="reference">
<a class="anchor" href="#reference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reference</h2>

<p><a href="https://developers.google.com/machine-learning/crash-course/training-neural-networks/video-lecture">https://developers.google.com/machine-learning/crash-course/training-neural-networks/video-lecture</a></p>

  </div>

<div>
  <p>이 섹션에서는 역전파의 실패 사례 및 신경망을 정규화하는 가장 일반적인 방법을 설명합니다.</p>

<h2 id="part-41eb7c0ee8859147">실패 사례</h2>

<p>몇 가지 일반적인 이유로 인해 역전파에서 문제가 나타날 수 있습니다.</p>

<h3 id="part-2f832346e53f9b31">경사 소실</h3>

<p>입력 쪽에 가까운 하위 레이어의 경사가 매우 작아질 수 있습니다.</p>

<p>심층 네트워크에서 이러한 경사를 계산할 때는 많은 작은 항의 곱을 구하는 과정이 포함될 수 있습니다.</p>

<p>하위 레이어의 경사가 <em>0</em> 에 가깝게 소실되면 이러한 레이어에서 <strong>학습 속도가 크게 저하되거나 학습이 중지</strong>됩니다.</p>

<p><code>ReLU</code> 활성화 함수를 통해 경사 소실을 방지할 수 있습니다.</p>

<h3 id="part-2f832346e5eb3cad">경사 발산</h3>

<p>네트워크에서 가중치(Weight)가 매우 크면 하위 레이어의 경사에 많은 큰 항의 곱이 포함됩니다.</p>

<p>이러한 경우 경사가 너무 커져서 수렴하지 못하고 발산하는 현상이 나타날 수 있습니다.</p>

<p><code>batch 정규화</code>를 사용하거나 학습률을 낮추면 경사 발산을 방지할 수 있습니다.</p>

<h3 id="relu">ReLU 유닛 소멸</h3>

<p><code>ReLU</code> 유닛의 가중 합이 <em>0</em> 미만으로 떨어지면 <code>ReLU</code> 유닛이 고착될 수 있습니다.</p>

<p>이러한 경우 활동이 출력되지 않으므로 네트워크의 출력에 어떠한 영향도 없으며 역전파 과정에서 경사가 더 이상 통과할 수 없습니다.<br>
이와 같이 경사의 근원이 단절되므로, 가중 합이 다시 <em>0</em> 이상으로 상승할 만큼 <code>ReLU</code>가 변화하지 못할 수도 있습니다.</p>

<p>학습률(Learning rate)을 낮추면 <code>ReLU</code> 유닛 소멸을 방지할 수 있습니다.</p>

<h2 id="part-a242d157453f1c7f">드롭아웃 정규화</h2>

<ul>
<li>또 하나의 정규화 형태, NN 에 유용</li>
<li>신경망에는 <strong>드롭아웃</strong>이라는 또 하나의 정규화 형태가 유용</li>
<li>이는 단일 경사 스텝에서 유닛 활동을 무작위로 배제하는 방식

<ul>
<li>앙상블 모델과의 접점</li>
</ul>
</li>
<li>드롭아웃을 반복할수록 정규화가 강력짐

<ul>
<li>0.0 = 드롭아웃 정규화 없음</li>
<li>1.0 = 전체 드롭아웃. 모델에서 학습을 수행 안함</li>
<li>0.0~1.0 범위 값 = 중간 범위의 값이 유용함</li>
</ul>
</li>
<li>그롭아웃으로 심층 신경망 (Deep learning) 이 주목받게 됨.</li>
</ul>

<blockquote>
<p><strong>주요 용어:</strong>
- <a href="https://developers.google.com/machine-learning/crash-course/glossary#activation_function" rel="nofollow" target="_blank">활성화 함수</a>:<br>
이전 레이어의 모든 입력에 대한 가중 합을 취하고 출력 값(일반적으로 비선형)을 생성하여 다음 레이어로 전달하는 <code>ReLU</code>, <code>시그모이드</code> 등의 함수입니다.
- <a href="https://developers.google.com/machine-learning/crash-course/glossary#backpropagation" rel="nofollow" target="_blank">역전파</a>:<br>
  - 신경망에서 <strong>경사하강법</strong> 을 수행하는 기본 알고리즘.
  - 우선, 정방향 단계에서 각 노드의 출력 값을 계산하고 캐시.
  - 그런 다음 역방향 단계에서 그래프를 통과하며 각 매개변수를 기준으로 오차의 편미분을 계산.
- <a href="https://developers.google.com/machine-learning/crash-course/glossary#dropout_regularization" rel="nofollow" target="_blank">드롭아웃 정규화</a>:
  - 신경망을 학습시키는 데 유용한 정규화 형태.
  - 사용시 단일 경사 스텝이 일어날 때마다 특정 네트워크 레이어의 유닛을 고정된 개수만큼 무작위로 선택하여 삭제.
  - 드롭아웃하는 유닛이 많을수록 정규화가 강력해짐.
  - 이 방식은 네트워크를 학습시켜 더 작은 네트워크로 이루어진 대규모 <strong>앙상블 기법(Ensemble)</strong> 을 모방하도록 하는 방식과 비슷합니다.
  - 자세한 내용은 드롭아웃: 신경망의 과적합을 방지하는 간단한 방법을 참조하세요.
    - 신경망 모델이 복잡해질 때 가중치 감소만으로는 어려운데 드롭아웃 기법은 뉴런의 연결을 임의로 삭제하는 것.
    - 훈련시 임의의 뉴런을 골라 삭제, 신호를 전달하지 않게 하기.
    - 테스트시 모든 뉴런을 사용.
- <a href="https://developers.google.com/machine-learning/crash-course/glossary#gradient_descent" rel="nofollow" target="_blank">경사하강법</a>:<br>
  - 학습 데이터의 조건에 따라 모델의 매개변수를 기준으로 손실의 경사를 계산하여 손실을 최소화하는 기법.
  - 쉽게 설명하면, 경사하강법은 매개변수를 반복적으로 조정하면서 손실을 최소화하는 가중치와 편향의 가장 적절한 조합을 점진적으로 찾는 방식.
- <a href="https://developers.google.com/machine-learning/crash-course/glossary#ReLU" rel="nofollow" target="_blank">정류 선형 유닛</a>(ReLU): 다음 규칙을 따르는 활성화 함수입니다.
  - 입력: 음수 또는 0 -&gt; 출력: 0
  - 입력: 양수 -&gt; 출력: 입력과 같음. </p>

<p><strong>가중치 감소(Weight Decay):</strong><br>
학습 중에 가중치가 큰 것에 대해서는 일종의 패널티를 부과해 과적합의 위험을 줄이는 방법이다.
가중치의 제곱 법칙(L2 법칙; 많이 사용된다)를 손실함수에 더해 손실함수 값이 더 커지게 한다.
그만큼 가중치가 커지는 것을 억제하기 되는 것이다.</p>

<p><strong>앙상블 기법(Ensemble):</strong>
서로 다른 모델들을 학습해서 개별 모델들에서 나온 출력의 평균을 내어 추론하는 학습 방식.
드롭아웃은 학습할 때 뉴런을 무작위로 학습해 매번 다른 모델들을 학습시킨다는 측면에서 앙상블 기법과 유사.</p>
</blockquote>

<h2 id="reference">Reference</h2>

<p><a href="https://developers.google.com/machine-learning/crash-course/training-neural-networks/video-lecture" rel="nofollow" target="_blank">https://developers.google.com/machine-learning/crash-course/training-neural-networks/video-lecture</a></p>

</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Portal2312&#39;s blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Portal2312&#39;s blog</li><li><a class="u-email" href="mailto:portal2312@gmail.com">portal2312@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/portal2312"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">portal2312</span></a></li><li><a href="https://www.twitter.com/portal2312"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">portal2312</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Welcome to my blog.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
