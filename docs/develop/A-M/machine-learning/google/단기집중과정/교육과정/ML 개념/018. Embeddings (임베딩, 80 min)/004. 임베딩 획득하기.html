<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Embeddings - Obtaining Embeddings (임베딩 - 임베딩 획득하기) | Portal2312&#39;s blog</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="Embeddings - Obtaining Embeddings (임베딩 - 임베딩 획득하기)" />
<meta name="author" content="mkkim" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="임베딩을 획득하는 방법은 Google에서 만든 최첨단 알고리즘을 포함하여 여러 가지 방법이 있습니다." />
<meta property="og:description" content="임베딩을 획득하는 방법은 Google에서 만든 최첨단 알고리즘을 포함하여 여러 가지 방법이 있습니다." />
<link rel="canonical" href="/blog/docs/develop/A-M/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/018.%20Embeddings%20(%EC%9E%84%EB%B2%A0%EB%94%A9,%2080%20min)/004.%20%EC%9E%84%EB%B2%A0%EB%94%A9%20%ED%9A%8D%EB%93%9D%ED%95%98%EA%B8%B0.html" />
<meta property="og:url" content="/blog/docs/develop/A-M/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/018.%20Embeddings%20(%EC%9E%84%EB%B2%A0%EB%94%A9,%2080%20min)/004.%20%EC%9E%84%EB%B2%A0%EB%94%A9%20%ED%9A%8D%EB%93%9D%ED%95%98%EA%B8%B0.html" />
<meta property="og:site_name" content="Portal2312&#39;s blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-12-02T16:38:51+09:00" />
<script type="application/ld+json">
{"description":"임베딩을 획득하는 방법은 Google에서 만든 최첨단 알고리즘을 포함하여 여러 가지 방법이 있습니다.","headline":"Embeddings - Obtaining Embeddings (임베딩 - 임베딩 획득하기)","dateModified":"2019-12-02T16:38:51+09:00","datePublished":"2019-12-02T16:38:51+09:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"/blog/docs/develop/A-M/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/018.%20Embeddings%20(%EC%9E%84%EB%B2%A0%EB%94%A9,%2080%20min)/004.%20%EC%9E%84%EB%B2%A0%EB%94%A9%20%ED%9A%8D%EB%93%9D%ED%95%98%EA%B8%B0.html"},"url":"/blog/docs/develop/A-M/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/018.%20Embeddings%20(%EC%9E%84%EB%B2%A0%EB%94%A9,%2080%20min)/004.%20%EC%9E%84%EB%B2%A0%EB%94%A9%20%ED%9A%8D%EB%93%9D%ED%95%98%EA%B8%B0.html","author":{"@type":"Person","name":"mkkim"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href='/blog/assets/main.css'><link type="application/atom+xml" rel="alternate" href="/blog/feed.xml" title="Portal2312's blog" /><script>
if(!(window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1")) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-113063601-1', 'auto');
  ga('send', 'pageview');
}
</script>
  
<script src='/blog/dist/js/common.bundle.js'></script>
</head>
<body><header class="site-header" role="banner">
  <div class="wrapper">
    <a class="site-title" rel="author" href="/blog/">Portal2312&#39;s blog</a>
    <nav class="site-nav">
    <input type="checkbox" id="nav-trigger" class="nav-trigger" />
    <label for="nav-trigger">
      <span class="menu-icon">
        <svg viewBox="0 0 18 15" width="18px" height="15px">
          <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
        </svg>
      </span>
    </label>

    <div class="trigger"><a class="page-link" href="/blog/about.html">
            About
          </a><a class="page-link" href="/blog/posts.html">
            Posts
          </a><a class="page-link" href="/blog/history.html">
            History
          </a><a class="page-link" href="/blog/docs/index.html">
            Docs
          </a></div>
  </nav>
  </div>
  <div class="scroll-indicator-container">
  <div class="scroll-indicator-bar" id="scrollIndicatorBar"></div>
</div>

</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <h1>Embeddings - Obtaining Embeddings (임베딩 - 임베딩 획득하기)</h1>

  <div>
    <h2>Table of contents</h2>
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#part-b6f126dcc5f2f643">표준 차원 축소 기법</a></li>
<li class="toc-entry toc-h2"><a href="#word2vec">Word2vec</a></li>
<li class="toc-entry toc-h2"><a href="#part-988a9e8aa57a0670">더 큰 모델의 일부로서 임베딩 학습</a></li>
<li class="toc-entry toc-h2"><a href="#reference">Reference</a></li>
</ul><p>임베딩을 획득하는 방법은 Google에서 만든 최첨단 알고리즘을 포함하여 여러 가지 방법이 있습니다.</p>

<h2 id="part-b6f126dcc5f2f643">
<a class="anchor" href="#part-b6f126dcc5f2f643" aria-hidden="true"><span class="octicon octicon-link"></span></a>표준 차원 축소 기법</h2>

<p>저차원 공간에서 고차원 공간의 중요한 구조를 캡처할 수 있는 여러 가지 수학적 기법이 존재합니다. 이론상 이 기법들은 어느 것이든 머신러닝 시스템용 임베딩을 만드는 데 사용할 수 있습니다.</p>

<p>예를 들어 <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">주성분 분석</a>(PCA)은 단어 임베딩을 만드는 데 사용되어 왔습니다.
BOW 벡터와 같은 인스턴스의 집합이 주어지면 PCA는 단일 차원으로 축소할 수 있는 높은 상관도를 갖는 차원을 찾습니다.</p>

<h2 id="word2vec">
<a class="anchor" href="#word2vec" aria-hidden="true"><span class="octicon octicon-link"></span></a>Word2vec</h2>

<ul>
<li>단어 임베딩 학습을 위해 Google 에서 개발한 알고리즘.</li>
<li>
<strong>분포 가설</strong>에 기반하여 의미론적으로 유사한 단어를 기하학적으로 가까운 임베딩 벡터로 매핑함.<br>

<ul>
<li>분포 가설은 주로 같은 단어가 인접하는 단어 간에는 의미론적으로 유사한 경향이 있다고 봄.</li>
<li>예를 들어 '개'와 '고양이'가 '수의사'라는 단어에 자주 인접한다는 사실은 이 두 단어 간에 의미론적 유사성이 있다는 것을 보여줌.</li>
<li>1957년 언어학자 John Firth가 '단어를 알려면 같이 다니는 친구 단어들을 알아야 해(You shall know a word by the company it keeps)'라고 말한 것처럼 말입니다.</li>
</ul>
</li>
<li>실제로 함께 등장하는 단어 그룹과 무작위로 그룹화된 단어를 구분하도록 신경망을 학습시켜 이와 같은 문맥상의 정보를 활용합니다.

<ul>
<li>입력 레이어는 하나 이상의 문맥 단어와 함께 대상 단어의 희소 표현을 취합니다.</li>
<li>이 입력은 더 작은 히든 레이어 하나에 연결됩니다.</li>
</ul>
</li>
</ul>

<p>이 알고리즘의 한 버전에서는 시스템이 대상 단어 대신 무작위의 노이즈 단어를 취하여 음의 예를 만듭니다. 양의 예로 '비행기가 난다'가 있으면 시스템은 '조깅'을 대신 넣어 대조되는 음의 예인 '조깅이 난다'를 만들 수 있습니다.</p>

<p>다른 알고리즘 버전에서는 실제 대상 단어와 무작위로 선택한 문맥 단어를 짝지어 음의 예를 만듭니다.
이를 통해 시스템은 양의 예인 (비행기, 가) (난다, 비행기)와 음의 예인 (편찬하다, 비행기) (누가, 비행기)를 취하여 어느 쌍이 텍스트에 실제로 함께 표시되었는지 구분하는 법을 학습합니다.</p>

<p>하지만 두 버전 모두 분류자가 진짜 목표인 것은 아닙니다.</p>

<p>모델을 학습한 후에야 임베딩을 획득하게 됩니다.
입력 레이어와 히든 레이어를 연결하는 가중치를 사용하여 단어의 희소 표현을 더 작은 벡터에 매핑할 수 있습니다.
이 임베딩은 다른 분류자에서 재사용할 수 있습니다.</p>

<p>word2vec의 자세한 내용은 <a href="https://www.tensorflow.org/tutorials/representation/word2vec">tensorflow.org</a>의 가이드를 참조하세요.</p>

<h2 id="part-988a9e8aa57a0670">
<a class="anchor" href="#part-988a9e8aa57a0670" aria-hidden="true"><span class="octicon octicon-link"></span></a>더 큰 모델의 일부로서 임베딩 학습</h2>

<p>임베딩을 대상 작업을 위한 신경망의 일부로서 학습할 수도 있습니다.
이 접근법은 특정 시스템에 맞게 임베딩을 효과적으로 맞춤화할 수 있지만 임베딩을 별도로 학습하는 것보다 시간이 오래 걸릴 수 있습니다.</p>

<p>일반적으로, 희소 데이터 또는 임베딩하려는 밀집 데이터가 있는 경우 크기가 d인 특수 유형의 은닉 단위인 임베딩 단위를 만들 수 있습니다.
이 임베딩 레이어는 다른 특성 및 히든 레이어와 결합할 수 있습니다.
모든 DNN(심층신경망)이 그러하듯, 최종 레이어는 최적화 중인 손실입니다.
예를 들어, 다른 사용자의 <em>관심분야</em>를 바탕으로 특정 사용자의 관심분야를 예측하기 위한 협업 필터링 작업을 한다고 가정합니다.
이 작업에서 사용자가 시청한 소수의 영화를 양의 라벨로 무작위로 따로 분류하여 지도 학습 문제로 모델링한 다음 소프트맥스 손실을 최적화할 수 있습니다.</p>

<p><img src="EmbeddingExample3-1.svg" alt="EmbeddingExample3-1.svg"></p>

<p>그림 5. 협업 필터링 데이터에서 영화 임베딩을 학습하는 샘플 DNN 아키텍처</p>

<p>또 다른 예로, DNN의 일부로서 부동산 광고에 나온 <em>단어</em>에 관해 임베딩 레이어를 만들어 주택 가격을 평가하려 할 때, 학습 데이터에 있는 알려진 주택 판매가를 라벨로 사용하여 L2 손실을 최적화할 수 있습니다.</p>

<p>d차원 임베딩을 학습할 때, 각 항목은 d차원 공간의 지점에 매핑되어 유사한 항목이 이 공간에서 서로 가까이 위치하게 됩니다.</p>

<p>그림 6은 임베딩 레이어에서 학습한 가중치와 기하학적 보기 간의 관계를 보여줍니다.
입력 노드와 d차원 임베딩 레이어에 있는 노드 간의 에지 가중치는 d개 축 각각의 좌표 값과 일치합니다.</p>

<p><img src="dnn-to-geometric-view.svg" alt="dnn-to-geometric-view.svg"></p>

<h2 id="reference">
<a class="anchor" href="#reference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reference</h2>

<p><a href="https://developers.google.com/machine-learning/crash-course/embeddings/obtaining-embeddings">https://developers.google.com/machine-learning/crash-course/embeddings/obtaining-embeddings</a></p>

  </div>

<div>
  <p>임베딩을 획득하는 방법은 Google에서 만든 최첨단 알고리즘을 포함하여 여러 가지 방법이 있습니다.</p>

<h2 id="part-b6f126dcc5f2f643">표준 차원 축소 기법</h2>

<p>저차원 공간에서 고차원 공간의 중요한 구조를 캡처할 수 있는 여러 가지 수학적 기법이 존재합니다. 이론상 이 기법들은 어느 것이든 머신러닝 시스템용 임베딩을 만드는 데 사용할 수 있습니다.</p>

<p>예를 들어 <a href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="nofollow" target="_blank">주성분 분석</a>(PCA)은 단어 임베딩을 만드는 데 사용되어 왔습니다.
BOW 벡터와 같은 인스턴스의 집합이 주어지면 PCA는 단일 차원으로 축소할 수 있는 높은 상관도를 갖는 차원을 찾습니다.</p>

<h2 id="word2vec">Word2vec</h2>

<ul>
<li>단어 임베딩 학습을 위해 Google 에서 개발한 알고리즘.</li>
<li>
<strong>분포 가설</strong>에 기반하여 의미론적으로 유사한 단어를 기하학적으로 가까운 임베딩 벡터로 매핑함.<br>

<ul>
<li>분포 가설은 주로 같은 단어가 인접하는 단어 간에는 의미론적으로 유사한 경향이 있다고 봄.</li>
<li>예를 들어 '개'와 '고양이'가 '수의사'라는 단어에 자주 인접한다는 사실은 이 두 단어 간에 의미론적 유사성이 있다는 것을 보여줌.</li>
<li>1957년 언어학자 John Firth가 '단어를 알려면 같이 다니는 친구 단어들을 알아야 해(You shall know a word by the company it keeps)'라고 말한 것처럼 말입니다.</li>
</ul>
</li>
<li>실제로 함께 등장하는 단어 그룹과 무작위로 그룹화된 단어를 구분하도록 신경망을 학습시켜 이와 같은 문맥상의 정보를 활용합니다.

<ul>
<li>입력 레이어는 하나 이상의 문맥 단어와 함께 대상 단어의 희소 표현을 취합니다.</li>
<li>이 입력은 더 작은 히든 레이어 하나에 연결됩니다.</li>
</ul>
</li>
</ul>

<p>이 알고리즘의 한 버전에서는 시스템이 대상 단어 대신 무작위의 노이즈 단어를 취하여 음의 예를 만듭니다. 양의 예로 '비행기가 난다'가 있으면 시스템은 '조깅'을 대신 넣어 대조되는 음의 예인 '조깅이 난다'를 만들 수 있습니다.</p>

<p>다른 알고리즘 버전에서는 실제 대상 단어와 무작위로 선택한 문맥 단어를 짝지어 음의 예를 만듭니다.
이를 통해 시스템은 양의 예인 (비행기, 가) (난다, 비행기)와 음의 예인 (편찬하다, 비행기) (누가, 비행기)를 취하여 어느 쌍이 텍스트에 실제로 함께 표시되었는지 구분하는 법을 학습합니다.</p>

<p>하지만 두 버전 모두 분류자가 진짜 목표인 것은 아닙니다.</p>

<p>모델을 학습한 후에야 임베딩을 획득하게 됩니다.
입력 레이어와 히든 레이어를 연결하는 가중치를 사용하여 단어의 희소 표현을 더 작은 벡터에 매핑할 수 있습니다.
이 임베딩은 다른 분류자에서 재사용할 수 있습니다.</p>

<p>word2vec의 자세한 내용은 <a href="https://www.tensorflow.org/tutorials/representation/word2vec" rel="nofollow" target="_blank">tensorflow.org</a>의 가이드를 참조하세요.</p>

<h2 id="part-988a9e8aa57a0670">더 큰 모델의 일부로서 임베딩 학습</h2>

<p>임베딩을 대상 작업을 위한 신경망의 일부로서 학습할 수도 있습니다.
이 접근법은 특정 시스템에 맞게 임베딩을 효과적으로 맞춤화할 수 있지만 임베딩을 별도로 학습하는 것보다 시간이 오래 걸릴 수 있습니다.</p>

<p>일반적으로, 희소 데이터 또는 임베딩하려는 밀집 데이터가 있는 경우 크기가 d인 특수 유형의 은닉 단위인 임베딩 단위를 만들 수 있습니다.
이 임베딩 레이어는 다른 특성 및 히든 레이어와 결합할 수 있습니다.
모든 DNN(심층신경망)이 그러하듯, 최종 레이어는 최적화 중인 손실입니다.
예를 들어, 다른 사용자의 <em>관심분야</em>를 바탕으로 특정 사용자의 관심분야를 예측하기 위한 협업 필터링 작업을 한다고 가정합니다.
이 작업에서 사용자가 시청한 소수의 영화를 양의 라벨로 무작위로 따로 분류하여 지도 학습 문제로 모델링한 다음 소프트맥스 손실을 최적화할 수 있습니다.</p>

<p><img src="EmbeddingExample3-1.svg" alt="EmbeddingExample3-1.svg"></p>

<p>그림 5. 협업 필터링 데이터에서 영화 임베딩을 학습하는 샘플 DNN 아키텍처</p>

<p>또 다른 예로, DNN의 일부로서 부동산 광고에 나온 <em>단어</em>에 관해 임베딩 레이어를 만들어 주택 가격을 평가하려 할 때, 학습 데이터에 있는 알려진 주택 판매가를 라벨로 사용하여 L2 손실을 최적화할 수 있습니다.</p>

<p>d차원 임베딩을 학습할 때, 각 항목은 d차원 공간의 지점에 매핑되어 유사한 항목이 이 공간에서 서로 가까이 위치하게 됩니다.</p>

<p>그림 6은 임베딩 레이어에서 학습한 가중치와 기하학적 보기 간의 관계를 보여줍니다.
입력 노드와 d차원 임베딩 레이어에 있는 노드 간의 에지 가중치는 d개 축 각각의 좌표 값과 일치합니다.</p>

<p><img src="dnn-to-geometric-view.svg" alt="dnn-to-geometric-view.svg"></p>

<h2 id="reference">Reference</h2>

<p><a href="https://developers.google.com/machine-learning/crash-course/embeddings/obtaining-embeddings" rel="nofollow" target="_blank">https://developers.google.com/machine-learning/crash-course/embeddings/obtaining-embeddings</a></p>

</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Portal2312&#39;s blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Portal2312&#39;s blog</li><li><a class="u-email" href="mailto:portal2312@gmail.com">portal2312@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/portal2312"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">portal2312</span></a></li><li><a href="https://www.twitter.com/portal2312"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">portal2312</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Welcome to my blog.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
