< wget >

네트워크 상에서 데이터를 다운로드 받는다. 유저와의 상호작용이 필요 없으므로 - non interactive - 자동화된 다운로더 시스템을 구성할 수 있다.

wget [option]... [URL]...

설명 

GNU wget은 상호작용을 필요로 하지 않는 다운로드 프로그램으로 Web(:12)으로 부터 파일들을 가져올 수 있다. HTTP, HTTPS, FTP 프로토콜을 지원하며, HTTP proxy에서 데이터를 가져올 수도 있다.

wget이 상호작용을 필요로 하지 않는다는 것은, 백그라운드 상태에서 작동할 수 있음을 의미한다. 사용자는 로그인(:12)을 하지 않은 상태에서도 cron이나 Damon을 이용 wget을 실행시킬 수 있다. 또한 cookie, Form action을 지원해서 사용자 액션을 시뮬레이션 할 수도 있다.

wget은 HTML과 XHTML 페이지를 다운로드 받아서 로컬 파일시스템에 저장한다. 이때 파일이름과 디렉토리 구조까지를 그대로 로컬시스템에 구축함으로써, backup, mirror 사이트 구축등에 사용할 수도 있다. 또한 recursive 다운로드를 지원해서, 사이트 전체를 쉽게 내려받을 수 있다.

wget은 느리거나 불안정한 네트워크 환경에서도 매우 잘 작동하는 견고한 프로그램이다. 네트워크 환경이 불안해서 도중에 연결이 끊겼다면, 연결이 끊긴 시점부터 다운로드 받는 기능도 가지고 있다.


ex) wget -nd -r ftp://ftp.ncbi.nlm.nih.gov/blast/db/
- nd : 디렉토리를 만들지 않는다. 계층적으로 나열된 웹 사이트의 디렉토리의 내용을 한 디렉토리로 불러올 때 편리하다. -r 옵션과 같이 사용하면 매우 유용하다.
설명 : 이렇게 하면 ftp://ftp.ncbi.nlm.nih.gov/blast/db/  내의 내용물을 현재 폴더에 몽땅 다운로드받는다.
- r : 지정된 디렉토리의 하위 디렉토리의 내용을 몽땅 다운로드한다.
설명 : 이렇게 하면 ftp://ftp.ncbi.nlm.nih.gov/blast/db/ 의 디렉토리 구조를 유지한 채로 모든 파일을 불러온다.

옵션 

Start Up 

-V, --version
wget의 버전을 보여주고 종료한다. 

-h, --help
도움말을 보여준다. 

-b, --background
백그라운드에서 작동하도록 한다. 

-e, --execute=COMMAND
.wgetrc를 읽어서 명령을 실행한다. 

Logging and input file 

-o, --output-file=FILE
메시지들을 파일로 남긴다. 

-a, --append-output=FILE
메시지들을 파일에 추가한다 

-d, --debug
디버그 내용을 출력한다. 

-q, --quiet
메시지들을 표준출력(:12)하지 않는다. 

-nv, --non-verbose
기본옵션을 제거한다. 

-i, --input-file=FILE
다운로드 받을 URL(:12)주소를 파일에서 찾는다. 

-F, --force-html
HTML과 같은 파일을 입력값으로 취급한다. 

-B, --base=URL, --sslcertfile=FILE, --sslcertkey=KEYFILE
파일이나 링크들의 SSL(:12)인증을 위해서 URL, keyfile을 사용한다. 

다운로드 

--bind-address=ADDRESS
로컬 호스트의 주소를 설정한다.
-t, --tries=NUMBER
설정한 숫자 만큼 다시 시도한다. 0은 무한대
-O, --output-document=FILE
문서를 파일로 쓴다. 

-c, --continue
연결이 끊긴 시점부터, 이어서 파일을 다운로드 받는다. 

-N, --timestamping
로컬에 있는 것보다, 새롭지 않으면 파일을 다시 받지 않는다. 

-S, --server-response
서버의 응답을 출력한다. 

--spider
다운로드 하지 않는다. 

-T, timeout=SECONDS
종료된 시간을 초단위로 설정한다. 

-w, --wait=SECONDS
연결을 위해 기다리는 시간을 초단위로 설정한다. 

-Y, --proxy=on/off
프락시 모드를 켜고 끈다. 

-Q, --quota=NUMBER
숫자만큼 분담하도록 설정한다. 

디렉토리 관리 

-nd, --no-directories
디렉토리를 생성하지 않는다. 

-x, --force-directories
디렉토리를 강제로 생성한다. 

-nH, --no-host-directories
호스트 이름의 디렉토리를 생성하지 않는다. 

-P, --directory-prefix=PREFIX
파일들이 저장될 디렉토리를 지정한다. 

HTTP 옵션 

--http-user=USER
http 유저 아이디를 설정한다.
--http-passwd=PASS
http 패스워드 암호를 설정한다. 

-C, --cache=on/off
서버측의 캐쉬 데이터를 허용하는게 보통이나, 제거할 수도 있다. 

-E, --html-extension
모든 text/html 형식의 문서들과 .html 파일을 같이 확장해서 저장한다. 

-s, --save-headers
파일에 HTTP헤더를 저장한다. 

-U, --user-agent=AGENT
Wget이 아닌, 다른 브라우저로 AGENT를 설정한다. 

--no-http-keep-alive
HTTP의 keep-alive를 제거한다. 

--cookies=off
cookie를 사용하지 않는다.
--load-cookies=FILE
쿠키의 값을 파일에서 읽어들인다. 

--save-cookies=FILE
세션이 끝난 이후 전달받은 cookie값을 저장한다. 

FTP 옵션 

-nr, --dont-remove-listing
.listing 파일에 있는건 지우지 않는다. 

-g, --glob=on/off
하나의 파일이름으로 할지를 결정한다. 

--passive-ftp
passive 모드로 전송한다. 

--retr-symlinks
복구중에 링크가 걸린 파일을 가져온다. 

Recursive retrieval 

-r, --recursive
웹데이터를 재귀적으로 가져온다. 대상 웹서버에 무리를 줄 수 있으니, 주의해서 사용해야 한다. 

-l, --level=NUMBER 

--delete-after
다운로드 받은후 로컬파일을 삭제한다. 

-k, convert-links
관계없는 링크를 관련있는 링크로 변환한다. 

-K, --backup-converted
변환하기 전에, 원본파일을 .orig 파일로 백업한다. 

-m, --mirror
-r -N -l inf -nr 옵션과 사용할 수 있다. 

-p, --page-requisites
모든 이미지와 HTML 페이지등을 가져온다. 

Recursive accept/reject 

-A, --accept=LIST
받아들일 것을 확장하기 위해서 콤마로 분리된 리스트로 받아들인다. 

-R, --reject=LIST
받아들이지 않을 것을 콤마로 분리된 리스트로 받아들인다. 

-D, --domains=LIST
받아들일 도메인을 콤마로 분리된 리스트로 받아들인다.
--exclude-domains=LIST
거절할 도메인을 콤마로 분리된 리스트로 받아들인다 

--follow-ftp
HTML 문서에서의 FTP 링크를 따라간다 

--follow-tags=LIST
HTML태그에서 따라갈 콤마로 분리된 리스트로 받아들인다. 

-G, --ignor-tags
무시할 HTML 태그를 콤마로 분리된 리스트로 받아들인다. 

-H, --span-hosts
반복할때에 다른 호스트로도 가게 한다. 

-L, --relative
관계있는 링크만 따라간다. 

-l, --include-directories=LIST
허용할 디렉토리를 리스트로 받아들인다. 

-nh, --no-host-lookup
DNS(:12)의 검색 호스트를 사용하지 않는다. 

-np, --no-parent
상위 디렉토리를 올라가지 않도록 한다. 

예제 

HTTPS 서버의 문서 가져오기 :  때때로 쿠키 값을 요구하는 경우가 있는데, 이때는 --load-cookie로 쿠키 파일을 가져올 수 있다. 이 쿠키파일은 mozilla 브라우저의 쿠키파일이다. HTTPS(:12) 서버의 상당수가 self-signed 인증서를 가지고 있는데, 이 경우 증명된 인증서가 아니라고 해서 문서를 읽어오지 않는다. 이럴때는 --no-check-certificate 옵션을 주면 된다.
# wget https://www.test.com:1111/index.php --load-cookie cookies.sqlite --no-check-certificate -O /dev/null






다음과 같은 2가지 사항에 대해서 체크한다면, 웹서비스에 대한 기본적인 품질 체크는 가능할 것이다.

1. 페이지 응답체크
2. 페이지 응답시간

Submit Process 

여기에 덧붙여서 Submit Process까지 체크한다면, 더 나은 품질 모니터링 환경을 만들 수 있을 것이다.

어떤 웹서비스가 제대로 작동하는지 확인하고 싶을 경우, POST(:12) 혹은 GET(:12)으로 연결된 몇개의 페이지를 연결해서 검사해야할 필요가 있다. 예를 들어서 로그인을 위해서 OpenID(:12)를 사용한다고 가정해보자.

기존의 로그인 방식이라면, 로그인관련 데이터가 로컬에 있으니, 로그인데이터를 관리하는 DB시스템이 제대로 살아있는지만 확인할 수 있으면 되었다. 그러나 OpenID와 같은 경우에는 로컬 DB를 이용하는게 아닌, 로그인 서비스를 이용하는 방식이기 때문에 실제 Submit을 해서 다음 페이지로 넘어가는지를 확인해 주어야 한다.

Submit 프로세스를 체크하기 위해서는 HTTP(:12)와 POST(:12), GET(:12)을 이용한 데이터 전달방식 그리고 cookie(:12)에 대해서 알고 있어야 할것이다. 여기에서는 그냥 wget(1)을 이용해서 간단하게 처리하는 방법에 대해서 알아보도록 하겠다.

# wget --load-cookies=cookies.txt --save-cookies=cookies.txt \
--post-data 'uname=myid&pass=mypass&op=login' \
'http://www.domain.com/auth.php'  -O /dev/null
아주 간단하다. --post-data 옵션을 이용하면, 해당 페이지에 POST 데이터를 넘길 수 있다. POST 데이터를 받은 웹서버는 인증과정을 거친 후, 그 결과를 Cookie로 클라이언트에 전달하고, 이후의 인증 세션유지는 cookie 값의 교환으로 이루어지게 된다. --save-cookies 옵션을 이용하면, 서버로 부터 넘어온 cookie 값을 파일에 저장할 수가 있다. 이제 --load-cookies 옵션을 이용해서 해당 웹페이지를 호출할 때, 쿠키도 같이 보내면 된다.

위의 명령을 쉘스크립트(:12) 형태로 만들어서 주기적으로 실행하고, 그 결과를 분석하는 걸로, 간단하게 Submit Process에 대한 체크를 할 수 있다. C(:12) 언어가 마음에 든다면, fork(:12) & exec(:12)를 이용한 실행코드를 만들어 낼 수도 있을 것이다. 위의 경우는 요청한 페이지정보를 /dev/null 로 보내고 있는데, 로그인 결과까지를 확인하고 싶다면, 입력받은 페이지를 스트링매칭 시키는 방법으로 분석해야 할 것이다.

응답시간 

응답시간 역시 wget을 이용해서 간단하게 해결할 수 있다. 더불어 -p옵션을 사용한다면, 해당 페이지에 링크되어 있는 이미지, 사운드, CSS(:12) 데이터들의 로딩시간까지 함께 체크할 수 있다.
# wget -p http://www.domain.com/index.php > /dev/null