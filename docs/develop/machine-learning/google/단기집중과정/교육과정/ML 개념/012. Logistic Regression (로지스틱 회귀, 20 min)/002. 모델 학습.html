<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>로지스틱 회귀 - 모델 학습 | Portal2312&#39;s blog</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="로지스틱 회귀 - 모델 학습" />
<meta name="author" content="mkkim" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="로지스틱 회귀의 손실 함수" />
<meta property="og:description" content="로지스틱 회귀의 손실 함수" />
<link rel="canonical" href="http://localhost:4000/blog/docs/develop/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/012.%20Logistic%20Regression%20(%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%20%ED%9A%8C%EA%B7%80,%2020%20min)/002.%20%EB%AA%A8%EB%8D%B8%20%ED%95%99%EC%8A%B5.html" />
<meta property="og:url" content="http://localhost:4000/blog/docs/develop/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/012.%20Logistic%20Regression%20(%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%20%ED%9A%8C%EA%B7%80,%2020%20min)/002.%20%EB%AA%A8%EB%8D%B8%20%ED%95%99%EC%8A%B5.html" />
<meta property="og:site_name" content="Portal2312&#39;s blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-11-06T09:31:12+09:00" />
<script type="application/ld+json">
{"description":"로지스틱 회귀의 손실 함수","headline":"로지스틱 회귀 - 모델 학습","dateModified":"2019-11-06T09:31:12+09:00","datePublished":"2019-11-06T09:31:12+09:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/docs/develop/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/012.%20Logistic%20Regression%20(%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%20%ED%9A%8C%EA%B7%80,%2020%20min)/002.%20%EB%AA%A8%EB%8D%B8%20%ED%95%99%EC%8A%B5.html"},"url":"http://localhost:4000/blog/docs/develop/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/012.%20Logistic%20Regression%20(%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%20%ED%9A%8C%EA%B7%80,%2020%20min)/002.%20%EB%AA%A8%EB%8D%B8%20%ED%95%99%EC%8A%B5.html","author":{"@type":"Person","name":"mkkim"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href='/blog/assets/main.css'><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/blog/feed.xml" title="Portal2312's blog" /><script src='/blog/dist/js/common.bundle.js'></script>
</head>
<body><header class="site-header" role="banner">
  <div class="wrapper">
    <a class="site-title" rel="author" href="/blog/">Portal2312&#39;s blog</a>
    <nav class="site-nav">
    <input type="checkbox" id="nav-trigger" class="nav-trigger" />
    <label for="nav-trigger">
      <span class="menu-icon">
        <svg viewBox="0 0 18 15" width="18px" height="15px">
          <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
        </svg>
      </span>
    </label>

    <div class="trigger"><a class="page-link" href="/blog/about.html">
            About
          </a><a class="page-link" href="/blog/posts.html">
            Posts
          </a><a class="page-link" href="/blog/history.html">
            History
          </a><a class="page-link" href="/blog/docs/index.html">
            Docs
          </a></div>
  </nav>
  </div>
  <div class="scroll-indicator-container">
  <div class="scroll-indicator-bar" id="scrollIndicatorBar"></div>
</div>

</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <h1>로지스틱 회귀 - 모델 학습</h1>

  <div>
    <h2>Table of contents</h2>
    <ul class="section-nav">
<li class="toc-entry toc-h3"><a href="#part-f8e119c69ceeb76b">로지스틱 회귀의 손실 함수</a></li>
<li class="toc-entry toc-h3"><a href="#part-3735d7d34a7403f0">로지스틱 회귀의 정규화</a></li>
<li class="toc-entry toc-h2"><a href="#reference">Reference</a></li>
</ul><h3 id="part-f8e119c69ceeb76b">
<a class="anchor" href="#part-f8e119c69ceeb76b" aria-hidden="true"><span class="octicon octicon-link"></span></a>로지스틱 회귀의 손실 함수</h3>

<p>선형 회귀의 손실 함수는 제곱 손실입니다.
로지스틱 회귀의 손실 함수는 <strong>로그 손실</strong>로 다음과 같이 정의됩니다:</p>

<p>$$
로그 손실 = \sum_{(x,y)\in D} -ylog(y') - (1 - y)log(1 - y')
$$</p>

<ul>
<li>(x,y)∈ D: 라벨이 있는 예(x,y 쌍)가 많이 포함된 데이터 세트.</li>
<li>y: 라벨이 있는 예의 라벨입니다. 로지스틱 회귀이므로 y 값은 모두 0 또는 1.</li>
<li>y': x의 특성 세트에 대한 예측 값(0~1 사이의 값)입니다.</li>
</ul>

<p>로그 손실 방정식은 정보 이론에서 말하는 섀넌의 <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">엔트로피 측정</a>과 밀접한 관련이 있습니다. 또한 <a href="https://en.wikipedia.org/wiki/Likelihood_function">우도 함수</a>의 음의 로그로 y의 <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli 분포</a>를 가정합니다.
실제로 손실 함수를 최소화하면 최대 우도 추정치가 생성됩니다.</p>

<blockquote>
<p><strong>우도 함수:</strong>
관측 데이터 주어진 가장 설득력있는 특정 파라미터 값을 식별하기 위해 사용된다.</p>
</blockquote>

<h3 id="part-3735d7d34a7403f0">
<a class="anchor" href="#part-3735d7d34a7403f0" aria-hidden="true"><span class="octicon octicon-link"></span></a>로지스틱 회귀의 정규화</h3>

<p><a href="https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/video-lecture?hl=ko">정규화</a>는 로지스틱 회귀 모델링에서 매우 중요.
정규화하지 않으면 로지스틱 회귀의 점근 특성이 고차원에서 계속 손실을 0으로 만들려고 시도.</p>

<p>결과적으로 대부분의 로지스틱 회귀 모델에서 모델 복잡성 줄이기 위한 두가지 전략:</p>

<ul>
<li>L2 정규화</li>
<li>조기 중단 (학습 단계 수 또는 학습률을 제한)</li>
</ul>

<p>L1 정규화에 관해서는 <a href="(https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/video-lecture?hl=ko)">후속 모듈</a> 참조.</p>

<p>각 예에 고유 ID를 할당하고 각 ID를 자체 특성에 매핑한다고 가정.
정규화 함수를 지정하지 않으면 모델이 완전히 과적합 됨.
모델이 모든 예에서 손실을 0으로 만들려고 하지만 0으로 만들지 않아 각 표시 특성의 가중치를 +무한대 또는 -무한대로 만들기 때문입니다.
한 예에서 하나만 발생하는 드문 교차가 아주 많은 경우, 특성 교차가 포함된 고차원 데이터에서 이러한 일이 발생할 수 있습니다.</p>

<p>다행히 L2나 조기 중단을 사용하면 이러한 문제가 발생하지 않습니다.</p>

<blockquote>
<p><strong>Summary</strong></p>

<ul>
<li>로지스틱 회귀 모델은 확률을 생성합니다.</li>
<li>로그 손실은 로지스틱 회귀의 손실 함수입니다.</li>
<li>로지스틱 회귀는 많은 실무자가 광범위하게 사용합니다.</li>
</ul>
</blockquote>

<h2 id="reference">
<a class="anchor" href="#reference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reference</h2>

<p><a href="https://developers.google.com/machine-learning/crash-course/logistic-regression/model-training?hl=ko">https://developers.google.com/machine-learning/crash-course/logistic-regression/model-training?hl=ko</a></p>

  </div>

<div>
  <h3 id="part-f8e119c69ceeb76b">로지스틱 회귀의 손실 함수</h3>

<p>선형 회귀의 손실 함수는 제곱 손실입니다.
로지스틱 회귀의 손실 함수는 <strong>로그 손실</strong>로 다음과 같이 정의됩니다:</p>

<p>$$
로그 손실 = \sum_{(x,y)\in D} -ylog(y') - (1 - y)log(1 - y')
$$</p>

<ul>
<li>(x,y)∈ D: 라벨이 있는 예(x,y 쌍)가 많이 포함된 데이터 세트.</li>
<li>y: 라벨이 있는 예의 라벨입니다. 로지스틱 회귀이므로 y 값은 모두 0 또는 1.</li>
<li>y': x의 특성 세트에 대한 예측 값(0~1 사이의 값)입니다.</li>
</ul>

<p>로그 손실 방정식은 정보 이론에서 말하는 섀넌의 <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" rel="nofollow" target="_blank">엔트로피 측정</a>과 밀접한 관련이 있습니다. 또한 <a href="https://en.wikipedia.org/wiki/Likelihood_function" rel="nofollow" target="_blank">우도 함수</a>의 음의 로그로 y의 <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution" rel="nofollow" target="_blank">Bernoulli 분포</a>를 가정합니다.
실제로 손실 함수를 최소화하면 최대 우도 추정치가 생성됩니다.</p>

<blockquote>
<p><strong>우도 함수:</strong>
관측 데이터 주어진 가장 설득력있는 특정 파라미터 값을 식별하기 위해 사용된다.</p>
</blockquote>

<h3 id="part-3735d7d34a7403f0">로지스틱 회귀의 정규화</h3>

<p><a href="https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/video-lecture?hl=ko" rel="nofollow" target="_blank">정규화</a>는 로지스틱 회귀 모델링에서 매우 중요.
정규화하지 않으면 로지스틱 회귀의 점근 특성이 고차원에서 계속 손실을 0으로 만들려고 시도.</p>

<p>결과적으로 대부분의 로지스틱 회귀 모델에서 모델 복잡성 줄이기 위한 두가지 전략:</p>

<ul>
<li>L2 정규화</li>
<li>조기 중단 (학습 단계 수 또는 학습률을 제한)</li>
</ul>

<p>L1 정규화에 관해서는 <a href="(https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/video-lecture?hl=ko)">후속 모듈</a> 참조.</p>

<p>각 예에 고유 ID를 할당하고 각 ID를 자체 특성에 매핑한다고 가정.
정규화 함수를 지정하지 않으면 모델이 완전히 과적합 됨.
모델이 모든 예에서 손실을 0으로 만들려고 하지만 0으로 만들지 않아 각 표시 특성의 가중치를 +무한대 또는 -무한대로 만들기 때문입니다.
한 예에서 하나만 발생하는 드문 교차가 아주 많은 경우, 특성 교차가 포함된 고차원 데이터에서 이러한 일이 발생할 수 있습니다.</p>

<p>다행히 L2나 조기 중단을 사용하면 이러한 문제가 발생하지 않습니다.</p>

<blockquote>
<p><strong>Summary</strong></p>

<ul>
<li>로지스틱 회귀 모델은 확률을 생성합니다.</li>
<li>로그 손실은 로지스틱 회귀의 손실 함수입니다.</li>
<li>로지스틱 회귀는 많은 실무자가 광범위하게 사용합니다.</li>
</ul>
</blockquote>

<h2 id="reference">Reference</h2>

<p><a href="https://developers.google.com/machine-learning/crash-course/logistic-regression/model-training?hl=ko" rel="nofollow" target="_blank">https://developers.google.com/machine-learning/crash-course/logistic-regression/model-training?hl=ko</a></p>

</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Portal2312&#39;s blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Portal2312&#39;s blog</li><li><a class="u-email" href="mailto:portal2312@gmail.com">portal2312@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/portal2312"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">portal2312</span></a></li><li><a href="https://www.twitter.com/portal2312"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">portal2312</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Welcome to my blog.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
