<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Embeddings - Translating to a Lower-Dimensional Space (임베딩 - 저차원 공간으로 변환) | Portal2312&#39;s blog</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="Embeddings - Translating to a Lower-Dimensional Space (임베딩 - 저차원 공간으로 변환)" />
<meta name="author" content="mkkim" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="고차원 데이터를 저차원 공간에 매핑하여 희소한 입력 데이터의 핵심 문제를 해결할 수 있습니다." />
<meta property="og:description" content="고차원 데이터를 저차원 공간에 매핑하여 희소한 입력 데이터의 핵심 문제를 해결할 수 있습니다." />
<link rel="canonical" href="http://localhost:4000/blog/docs/develop/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/018.%20Embeddings%20(%EC%9E%84%EB%B2%A0%EB%94%A9,%2080%20min)/003.%20%EC%A0%80%EC%B0%A8%EC%9B%90%20%EA%B3%B5%EA%B0%84%EC%9C%BC%EB%A1%9C%20%EB%B3%80%ED%99%98.html" />
<meta property="og:url" content="http://localhost:4000/blog/docs/develop/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/018.%20Embeddings%20(%EC%9E%84%EB%B2%A0%EB%94%A9,%2080%20min)/003.%20%EC%A0%80%EC%B0%A8%EC%9B%90%20%EA%B3%B5%EA%B0%84%EC%9C%BC%EB%A1%9C%20%EB%B3%80%ED%99%98.html" />
<meta property="og:site_name" content="Portal2312&#39;s blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-11-06T09:31:12+09:00" />
<script type="application/ld+json">
{"description":"고차원 데이터를 저차원 공간에 매핑하여 희소한 입력 데이터의 핵심 문제를 해결할 수 있습니다.","headline":"Embeddings - Translating to a Lower-Dimensional Space (임베딩 - 저차원 공간으로 변환)","dateModified":"2019-11-06T09:31:12+09:00","datePublished":"2019-11-06T09:31:12+09:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/docs/develop/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/018.%20Embeddings%20(%EC%9E%84%EB%B2%A0%EB%94%A9,%2080%20min)/003.%20%EC%A0%80%EC%B0%A8%EC%9B%90%20%EA%B3%B5%EA%B0%84%EC%9C%BC%EB%A1%9C%20%EB%B3%80%ED%99%98.html"},"url":"http://localhost:4000/blog/docs/develop/machine-learning/google/%EB%8B%A8%EA%B8%B0%EC%A7%91%EC%A4%91%EA%B3%BC%EC%A0%95/%EA%B5%90%EC%9C%A1%EA%B3%BC%EC%A0%95/ML%20%EA%B0%9C%EB%85%90/018.%20Embeddings%20(%EC%9E%84%EB%B2%A0%EB%94%A9,%2080%20min)/003.%20%EC%A0%80%EC%B0%A8%EC%9B%90%20%EA%B3%B5%EA%B0%84%EC%9C%BC%EB%A1%9C%20%EB%B3%80%ED%99%98.html","author":{"@type":"Person","name":"mkkim"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href='/blog/assets/main.css'><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/blog/feed.xml" title="Portal2312's blog" /><script src='/blog/dist/js/common.bundle.js'></script>
</head>
<body><header class="site-header" role="banner">
  <div class="wrapper">
    <a class="site-title" rel="author" href="/blog/">Portal2312&#39;s blog</a>
    <nav class="site-nav">
    <input type="checkbox" id="nav-trigger" class="nav-trigger" />
    <label for="nav-trigger">
      <span class="menu-icon">
        <svg viewBox="0 0 18 15" width="18px" height="15px">
          <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
        </svg>
      </span>
    </label>

    <div class="trigger"><a class="page-link" href="/blog/about.html">
            About
          </a><a class="page-link" href="/blog/posts.html">
            Posts
          </a><a class="page-link" href="/blog/history.html">
            History
          </a><a class="page-link" href="/blog/docs/index.html">
            Docs
          </a></div>
  </nav>
  </div>
  <div class="scroll-indicator-container">
  <div class="scroll-indicator-bar" id="scrollIndicatorBar"></div>
</div>

</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <h1>Embeddings - Translating to a Lower-Dimensional Space (임베딩 - 저차원 공간으로 변환)</h1>

  <div>
    <h2>Table of contents</h2>
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#part-4886b6665c98d8f5">망 축소하기</a></li>
<li class="toc-entry toc-h2"><a href="#part-c62a5891322e4e7b">검색표로서의 임베딩</a>
<ul>
<li class="toc-entry toc-h3"><a href="#part-d9e1b7ea68d4ee6f">행렬 곱셈으로서의 임베딩 검색</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#reference">Reference</a></li>
</ul><p>고차원 데이터를 저차원 공간에 매핑하여 희소한 입력 데이터의 핵심 문제를 해결할 수 있습니다.</p>

<p>종이 실습에서 볼 수 있듯이, 작은 다차원 공간에서도 의미론적으로 유사한 항목은 한데 묶고 유사하지 않은 항목은 서로 떨어뜨리는 작업을 자유롭게 수행할 수 있습니다.
벡터 공간의 위치(거리와 방향)는 좋은 임베딩을 통해 의미론을 인코딩할 수 있습니다.
예를 들어 아래에 나온 실제 임베딩의 시각화 자료는 국가와 수도 간의 관계와 같은 의미론적인 관계를 캡처하는 기하학적인 관계를 나타냅니다.</p>

<p><img src="linear-relationships.svg" alt="linear-relationships.svg"></p>

<p>그림 4. 임베딩은 우수한 유추를 제공할 수 있습니다.</p>

<p>이러한 종류의 의미있는 공간은 머신러닝 시스템이 패턴을 감지하여 학습 작업을 향상 시킬 수 있는 기회를 제공합니다.</p>

<h2 id="part-4886b6665c98d8f5">
<a class="anchor" href="#part-4886b6665c98d8f5" aria-hidden="true"><span class="octicon octicon-link"></span></a>망 축소하기</h2>

<p>풍부한 의미론적 관계를 인코딩하기에 충분한 차원이 필요하지만 그와 동시에 시스템을 더 빠르게 학습할 수 있게 할만큼의 작은 임베딩 공간도 필요합니다.
유용한 임베딩은 대략 수백 차원에 달할 수 있습니다.
이 크기는 자연어 작업에 필요한 어휘의 크기보다 10의 몇 승만큼이나 더 작습니다.</p>

<h2 id="part-c62a5891322e4e7b">
<a class="anchor" href="#part-c62a5891322e4e7b" aria-hidden="true"><span class="octicon octicon-link"></span></a>검색표로서의 임베딩</h2>

<p><a href="https://developers.google.com/machine-learning/crash-course/glossary#embeddings">임베딩</a>은 하나의 행렬이고, 행렬의 각 열은 어휘 항목 하나에 대응합니다.<br>
단일 어휘 항목에 대한 밀집 벡터를 얻으려면 해당 항목에 대응하는 열을 검색합니다.</p>

<p>하지만 희소한 BOW(bag of words) 벡터는 어떻게 변환해야 할까요?
여러 개의 어휘 항목(예: 문장 또는 단락의 모든 단어)을 나타내는 희소 벡터에 대한 밀집 벡터를 얻으려면 개별 항목에 대해 임베딩을 검색한 다음 이를 전부 더하면 됩니다.</p>

<p>희소 벡터에 어휘 항목의 수가 포함되어 있으면 각 임베딩에 해당 항목의 수를 곱한 다음 이를 합계에 추가할 수 있습니다.</p>

<p>어디에선가 해본 것 같지 않나요?</p>

<h3 id="part-d9e1b7ea68d4ee6f">
<a class="anchor" href="#part-d9e1b7ea68d4ee6f" aria-hidden="true"><span class="octicon octicon-link"></span></a>행렬 곱셈으로서의 임베딩 검색</h3>

<p>방금 설명한 검색, 곱셈, 덧셈 절차는 행렬 곱셈과 동일합니다. 1xN 크기의 희소 표현 S와 NxM 크기의 임베딩 표 E가 주어지면 행렬 곱셈 SxE를 통해 1xM 밀집 벡터를 얻을 수 있습니다.</p>

<p>하지만 애초에 E는 어떻게 얻어야 할까요? 다음 섹션에서는 임베딩을 얻는 방법에 대해 살펴보겠습니다.</p>

<h2 id="reference">
<a class="anchor" href="#reference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reference</h2>

<p><a href="https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space">https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space</a></p>

  </div>

<div>
  <p>고차원 데이터를 저차원 공간에 매핑하여 희소한 입력 데이터의 핵심 문제를 해결할 수 있습니다.</p>

<p>종이 실습에서 볼 수 있듯이, 작은 다차원 공간에서도 의미론적으로 유사한 항목은 한데 묶고 유사하지 않은 항목은 서로 떨어뜨리는 작업을 자유롭게 수행할 수 있습니다.
벡터 공간의 위치(거리와 방향)는 좋은 임베딩을 통해 의미론을 인코딩할 수 있습니다.
예를 들어 아래에 나온 실제 임베딩의 시각화 자료는 국가와 수도 간의 관계와 같은 의미론적인 관계를 캡처하는 기하학적인 관계를 나타냅니다.</p>

<p><img src="linear-relationships.svg" alt="linear-relationships.svg"></p>

<p>그림 4. 임베딩은 우수한 유추를 제공할 수 있습니다.</p>

<p>이러한 종류의 의미있는 공간은 머신러닝 시스템이 패턴을 감지하여 학습 작업을 향상 시킬 수 있는 기회를 제공합니다.</p>

<h2 id="part-4886b6665c98d8f5">망 축소하기</h2>

<p>풍부한 의미론적 관계를 인코딩하기에 충분한 차원이 필요하지만 그와 동시에 시스템을 더 빠르게 학습할 수 있게 할만큼의 작은 임베딩 공간도 필요합니다.
유용한 임베딩은 대략 수백 차원에 달할 수 있습니다.
이 크기는 자연어 작업에 필요한 어휘의 크기보다 10의 몇 승만큼이나 더 작습니다.</p>

<h2 id="part-c62a5891322e4e7b">검색표로서의 임베딩</h2>

<p><a href="https://developers.google.com/machine-learning/crash-course/glossary#embeddings" rel="nofollow" target="_blank">임베딩</a>은 하나의 행렬이고, 행렬의 각 열은 어휘 항목 하나에 대응합니다.<br>
단일 어휘 항목에 대한 밀집 벡터를 얻으려면 해당 항목에 대응하는 열을 검색합니다.</p>

<p>하지만 희소한 BOW(bag of words) 벡터는 어떻게 변환해야 할까요?
여러 개의 어휘 항목(예: 문장 또는 단락의 모든 단어)을 나타내는 희소 벡터에 대한 밀집 벡터를 얻으려면 개별 항목에 대해 임베딩을 검색한 다음 이를 전부 더하면 됩니다.</p>

<p>희소 벡터에 어휘 항목의 수가 포함되어 있으면 각 임베딩에 해당 항목의 수를 곱한 다음 이를 합계에 추가할 수 있습니다.</p>

<p>어디에선가 해본 것 같지 않나요?</p>

<h3 id="part-d9e1b7ea68d4ee6f">행렬 곱셈으로서의 임베딩 검색</h3>

<p>방금 설명한 검색, 곱셈, 덧셈 절차는 행렬 곱셈과 동일합니다. 1xN 크기의 희소 표현 S와 NxM 크기의 임베딩 표 E가 주어지면 행렬 곱셈 SxE를 통해 1xM 밀집 벡터를 얻을 수 있습니다.</p>

<p>하지만 애초에 E는 어떻게 얻어야 할까요? 다음 섹션에서는 임베딩을 얻는 방법에 대해 살펴보겠습니다.</p>

<h2 id="reference">Reference</h2>

<p><a href="https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space" rel="nofollow" target="_blank">https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space</a></p>

</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Portal2312&#39;s blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Portal2312&#39;s blog</li><li><a class="u-email" href="mailto:portal2312@gmail.com">portal2312@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/portal2312"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">portal2312</span></a></li><li><a href="https://www.twitter.com/portal2312"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">portal2312</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Welcome to my blog.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
